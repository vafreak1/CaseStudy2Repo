{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d6fef",
   "metadata": {},
   "source": [
    "## First, we slice the train images into 31 x 31 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndigit(n, x):\n",
    "    x = str(x)\n",
    "    while(len(x) < n):\n",
    "        x = \"0\" + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(res, files = 20, test = False):\n",
    "    j = 0\n",
    "    if (test == True):\n",
    "        path = \"02\"\n",
    "    else:\n",
    "        path = \"train\"\n",
    "    res = int((res-1)/2)\n",
    "    \n",
    "    for n in range(files):\n",
    "        image = np.load(f\"images_{path}/images/image_{ndigit(3, n)}.npy\")\n",
    "        masks = np.load(f\"masks_{path}/masks/mask_{ndigit(3, n)}.npy\")\n",
    "        masks = np.reshape(masks, (1024,1024,1))\n",
    "        ground_truths_pos = np.array(np.where(masks != 0)).T\n",
    "\n",
    "        # Add padding to every image edge in case there are ground truths which are too close to an edge\n",
    "        padded_image = np.pad(image, ((0, 0), (res, res), (res, res)), mode='constant') \n",
    "        \n",
    "        # Slice and save image\n",
    "        for i in ground_truths_pos: \n",
    "            train_slice = (padded_image[:, i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1], np.array(masks[i[0], i[1], 0]))\n",
    "            np.save(f\"images_{path}/train/train_{ndigit(5, j)}.npy\", train_slice)                                 \n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_data(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca66217",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), # Converts an image to a Tensor\n",
    "     transforms.ConvertImageDtype(torch.float),\n",
    "     transforms.Lambda(lambda x : x / 3000),\n",
    "     transforms.Lambda(lambda x : 1 if x > 1 else x), # clip images between 0 and 1\n",
    "     transforms.Normalize((0.5)*12, # Mean for RGB\n",
    "                          (0.5)*12) # Std for RGB\n",
    "     ]) \n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'images_train/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "trainset0 = [np.load(file_path, allow_pickle=True) for file_path in file_paths]\n",
    "#delete every image that doesnt have the correct shape (!THIS MIGHT BE A REAL PROBLEM THAT NEEDS TO BE FIXED PROPERLY LATER!)\n",
    "trainset = []\n",
    "for pic in trainset0:\n",
    "    if pic[0].shape == (10,31,31):\n",
    "        trainset.append(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_channels(trainset, veggie, moisture):\n",
    "# structure of the data: \n",
    "#trainset[pic_no][0][CHANNEL][Horizontal][VERTIKAL] -> Intensity \n",
    "#trainset[pic_no][1]-> Ground truth \n",
    "    print(\"Shape vorher: Liste mit 10,31,31 Bildern\")\n",
    "    counter = 0\n",
    "    trainset = trainset\n",
    "\n",
    "    if veggie:\n",
    "        pic_no = 0\n",
    "        for pic in trainset:\n",
    "            counter += 1\n",
    "            pixel_values = pic[0][:][:]\n",
    "            channel8 = pixel_values[6]\n",
    "            channel4 = pixel_values[2]\n",
    "            channels = pic[0].shape[0]\n",
    "            #print(\"Chanels:\", channels)\n",
    "            width = pic[0].shape[1]\n",
    "            height = pic[0].shape[2]\n",
    "            vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(channel8, channel4))\n",
    "            trainset_transformed = np.append(trainset[pic_no][0], vegetation_array)\n",
    "            trainset1_transformed = np.reshape(trainset_transformed, (channels + 1, width, height))\n",
    "            trainset[pic_no] = (trainset1_transformed, trainset[pic_no][1])\n",
    "            pic_no += 1\n",
    "\n",
    "        print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "\n",
    "    if moisture:\n",
    "        pic_no = 0\n",
    "        for pic in trainset:\n",
    "            pixel_values = pic[0][:][:]\n",
    "            channel8a = pixel_values[7]\n",
    "            channel11 = pixel_values[8]\n",
    "            channels = pic[0].shape[0]\n",
    "            #print(\"Chanels:\", channels)\n",
    "\n",
    "            width = pic[0].shape[1]\n",
    "            height = pic[0].shape[2]\n",
    "            moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(channel8a, channel11))\n",
    "            trainset_transformed = np.append(trainset[pic_no][0], moisture_array)\n",
    "            trainset1_transformed = np.reshape(trainset_transformed, (channels + 1, width, height))\n",
    "            print(trainset1_transformed.shape)\n",
    "            trainset[pic_no] = (trainset1_transformed, trainset[pic_no][1]) # append ground truth in tupel \n",
    "            pic_no += 1\n",
    "\n",
    "        print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "\n",
    "    return trainset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a187451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "\n",
    "        # Extract the image from the tuple\n",
    "        image = item[0]\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return the transformed image and the remaining items in the tuple\n",
    "        return image, *item[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56934585",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = enrich_channels(trainset, True, True)\n",
    "#trainset[pic_no][0][channel][h][w] -> pixel value\n",
    "#trainset[pic_no][1] -> Ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sizes of the training set and validation set\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "\n",
    "# Split trainset into trainset and valset\n",
    "trainset, valset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for the training set and validation set\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89572caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = trainset[i]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, y\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "    def __init__(self, *layers, classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = 0.01  # Assign the learning rate here\n",
    "        self.classes = classes\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)  # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_prefix='train'): # New !\n",
    "        X, y = batch # Tuple with (X,y) in our case\n",
    "        y_hat = self(X)\n",
    "        loss = nn.MSELoss(y_hat, y)\n",
    "        self.log(f\"{log_prefix}_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx): # New!\n",
    "        with torch.no_grad():\n",
    "            return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam with Weight Decay (Most commonly used)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        # Simplest scheduler is ReduceLROnPlateau. This scheduler reduces the learning rate by 0.1\n",
    "        # if the val_loss has not decreased within the last 10 epochs.\n",
    "        scheduler = {\n",
    "            # REQUIRED: The scheduler instance\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "            # The unit of the scheduler's step size, could also be 'step'.\n",
    "            # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "            # updates it after a optimizer update.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"val_loss\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated, thus stopping\n",
    "            # training if not found. If set to `False`, it will only produce a warning\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, 'lr-scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements entry to SepConv2d, see Lang et al. (2019), p. 6\n",
    "class MyEntryLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_out = nn.Conv2d(in_channels, out_channels[len(out_channels)-1], (1,1))\n",
    "\n",
    "        self.entry_blocks = nn.ModuleList()\n",
    "        for i in range(len(out_channels)):\n",
    "            self.entry_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels[i], (1, 1)),\n",
    "                nn.BatchNorm2d(out_channels[i]),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "            in_channels = out_channels[i]  # Update in_channels for next iteration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_entry = x\n",
    "        for i in range(len(self.out_channels)):\n",
    "            x_entry = self.entry_blocks[i](x_entry)\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements SepConv2D\n",
    "class MySepConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "\n",
    "        self.sep_conv_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, groups=in_channels, **kwargs), # depthwise SepConv\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs), # pointwise SepConv\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_sep_conv = self.sep_conv_block(x)\n",
    "        x_sep_conv_2 = self.sep_conv_block(x_sep_conv) # performs second SepConv, see Lang et al. (2019), p. 6\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_sep_conv_2) # adds original input and sep_conv_2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = MyCNNModel(\n",
    "    MyEntryLayer(10, [128, 256, 512]), # increase number of channels to 512\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    MySepConvLayer(512, 512, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(512, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='32', max_epochs=1,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=50),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707927a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(tree_model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(tree_model, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900cc27",
   "metadata": {},
   "source": [
    "## Now, we can apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = testset[i]\n",
    "    y_hat = tree_model.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, testset.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
