{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "#import lightning.pytorch as pl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we slice the train images into 31 x 31 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ndigit(n, x):\n",
    "#     x = str(x)\n",
    "#     while(len(x) < n):\n",
    "#         x = \"0\" + x\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(res, files = 20, test = False):\n",
    "#     j = 0\n",
    "#     if (test == True):\n",
    "#         path = \"02\"\n",
    "#     else:\n",
    "#         path = \"train\"\n",
    "#     res = int((res-1)/2)\n",
    "    \n",
    "#     for n in range(files):\n",
    "#         image = np.load(f\"images_{path}/images/image_{ndigit(3, n)}.npy\")\n",
    "#         masks = np.load(f\"masks_{path}/masks/mask_{ndigit(3, n)}.npy\")\n",
    "#         masks = np.reshape(masks, (1024,1024,1))\n",
    "#         ground_truths_pos = np.array(np.where(masks != 0)).T\n",
    "#         for i in ground_truths_pos:\n",
    "#             train = (image[:, i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1], masks[i[0], i[1], 0])\n",
    "#             np.save(f\"images_{path}/train/train_{ndigit(5, j)}.npy\", train)\n",
    "#             j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fce53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_data(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca66217",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), # Converts an image to a Tensor\n",
    "     transforms.ConvertImageDtype(torch.float),\n",
    "     transforms.Normalize((0.5)*10, # Mean for RGB\n",
    "                          (0.5)*10) # Std for RGB\n",
    "     ]) \n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'images_train/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "trainset = [np.load(file_path, allow_pickle=True) for file_path in file_paths]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7764c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_image_resolution(trainset):\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "\n",
    "    # Find maximum width and height among all images in the trainset -> 32 in this case \n",
    "    for pic in trainset:\n",
    "        width = pic[0].shape[1]\n",
    "        height = pic[0].shape[2]\n",
    "        max_width = max(max_width, width)\n",
    "        max_height = max(max_height, height)\n",
    "\n",
    "    new_trainset = []\n",
    "\n",
    "    # Adjust the resolution of each image\n",
    "    for pic in trainset:\n",
    "        width = pic[0].shape[1]\n",
    "        height = pic[0].shape[2]\n",
    "\n",
    "        # Calculate padding for width and height dimensions\n",
    "        width_padding = max_width - width\n",
    "        height_padding = max_height - height\n",
    "\n",
    "        # Pad the image with zeros to match the maximum dimensions\n",
    "        padded_image = np.pad(pic[0], ((0, 0), (0, width_padding), (0, height_padding)), mode='constant')\n",
    "\n",
    "        # Reshape the image to have a shape of 10x31x31\n",
    "        reshaped_image = np.reshape(padded_image, (10, max_width, max_height))\n",
    "\n",
    "        # Update the image in the trainset\n",
    "        updated_image = (reshaped_image, pic[1])\n",
    "        new_trainset.append(updated_image)\n",
    "\n",
    "    return new_trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ec82b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_only_pixel = adjust_image_resolution(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "74106910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_channels(veggie, moisture):\n",
    "    counter = 0\n",
    "    trainset1 = trainset_only_pixel \n",
    "# structure of the data: \n",
    "#trainset[pic_no][0][CHANNEL][Horizontal][VERTIKAL] -> Intensity \n",
    "#trainset[pic_no][1]-> Ground truth \n",
    "    if(veggie):\n",
    "        pic_no = 0\n",
    "        for pic in trainset1: \n",
    "                counter += 1\n",
    "                pixel_values = pic[0][:][:]  \n",
    "                channel8 = pixel_values[6]\n",
    "                channel4 = pixel_values[2]\n",
    "                channels = pic[0].shape[0]\n",
    "                width = pic[0].shape[1]\n",
    "                height = pic[0].shape[2]\n",
    "                vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(channel8, channel4))\n",
    "                trainset_transformed = np.append(trainset1[pic_no][0],(vegetation_array))\n",
    "                #print(\"added \", vegetation_array.size, \"veg values to a total length of\" ,trainset_transformed.size )\n",
    "                trainset1_transformed = np.reshape(trainset_transformed, (channels + 1,width,height))\n",
    "                \n",
    "                pic_no += 1\n",
    "        print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "                \n",
    "        if(moisture):\n",
    "        \n",
    "            pic_no = 0\n",
    "            for pic in trainset1: \n",
    "        \n",
    "                    pixel_values = pic[0][:][:]\n",
    "                    # different channels obv\n",
    "                    channel8a = pixel_values[7]\n",
    "                    channel11 = pixel_values[8]\n",
    "        \n",
    "                    channels = pic[0].shape[0]\n",
    "                    width = pic[0].shape[1]\n",
    "                    height = pic[0].shape[2]\n",
    "        \n",
    "                    #print(vegetation_array.size, channels, width, height)\n",
    "                    vegetation_array = np.divide((np.subtract(channel8a, channel11)), np.add(channel8a,channel11 ))\n",
    "                    trainset_transformed = np.append(trainset1[pic_no][0],(vegetation_array))\n",
    "                    trainset1_transformed = np.reshape(trainset_transformed, (channels +1,width,height))\n",
    "                    pic_no += 1\n",
    "                    \n",
    "            print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "            \n",
    "        return trainset1_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values_3k(): \n",
    "    \n",
    "    for pic in trainset:\n",
    "    np.place(pic[0], pic[0] > 3000, 1)\n",
    "    return trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "3f89f9e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[374], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainset_only_pixel \u001b[38;5;241m=\u001b[39m \u001b[43menrich_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ground_truths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pic \u001b[38;5;129;01min\u001b[39;00m trainset:\n",
      "Cell \u001b[0;32mIn[352], line 15\u001b[0m, in \u001b[0;36menrich_channels\u001b[0;34m(veggie, moisture)\u001b[0m\n\u001b[1;32m     13\u001b[0m channel4 \u001b[38;5;241m=\u001b[39m pixel_values[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     14\u001b[0m channels \u001b[38;5;241m=\u001b[39m pic[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[43mpic\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m height \u001b[38;5;241m=\u001b[39m pic[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     17\u001b[0m vegetation_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide((np\u001b[38;5;241m.\u001b[39msubtract(channel8, channel4)), np\u001b[38;5;241m.\u001b[39madd(channel8, channel4))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "trainset_only_pixel = enrich_channels(True, True)\n",
    "\n",
    "ground_truths = []\n",
    "for pic in trainset:\n",
    "    ground_truths.append(pic[1])\n",
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "58cc7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values_3k(trainset): \n",
    "    \n",
    "    for pic in trainset:\n",
    "        np.place(pic[0], pic[0] > 3000, 1)\n",
    "    return trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ca7e88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_only_pixel = filter_values_3k(trainset_only_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n",
      "(31, 31, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7130 into shape (31,31,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pic \u001b[38;5;129;01min\u001b[39;00m trainset:\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;66;03m#hier gibt es wohl bilder, die ein anderes shape haben, EORR \\n\",\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m         pic[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(pic[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m         pic[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m transform(pic[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7130 into shape (31,31,10)"
     ]
    }
   ],
   "source": [
    "## hier muss noch x und y zusammengeführt werden \n",
    "#x = trainset_only_pixel y = ground_truths-> Im array war mir das zu fummelig  \n",
    "\n",
    "for pic in trainset:\n",
    "        #hier gibt es wohl bilder, die ein anderes shape haben, EORR \\n\",\n",
    "        pic[0] = np.reshape(pic[0], (31,31,10))\n",
    "        print(pic[0].shape)\n",
    "        pic[0] = transform(pic[0])\n",
    "\n",
    "# Split train set into new train set and validation set\n",
    "val_size = round(0.2 * len(trainset))\n",
    "train_size = round(0.8 * len(trainset))\n",
    "trainset, valset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=0, transform=transform)\n",
    "validloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=0, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479096f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = ...\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89572caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m f, axarr \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m12\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     X,y \u001b[39m=\u001b[39m trainset[i]\n\u001b[0;32m      4\u001b[0m     X,y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m, y\n\u001b[0;32m      5\u001b[0m     axarr[i]\u001b[39m.\u001b[39mimshow(X)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = trainset[i]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, y\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "def forward(self, X):\n",
    "    return self.layers(X)\n",
    "\n",
    "def predict(self, X):\n",
    "    with torch.no_grad():\n",
    "        y_hat = self(X).argmax(1)\n",
    "    if self.classes is not None:\n",
    "        y_hat = [self.classes[i] for i in y_hat]\n",
    "    return y_hat\n",
    "\n",
    "def training_step(self, batch, batch_idx, log_prefix='train'): # New !\n",
    "    X, y = batch # Tuple with (X,y) in our case\n",
    "    y_hat = self(X)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    self.log(f\"{log_prefix}_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "def validation_step(self, batch, batch_idx): # New!\n",
    "    with torch.no_grad():\n",
    "        return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    # Adam with Weight Decay (Most commonly used)\n",
    "    optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "    # Simplest scheduler is ReduceLROnPlateau. This scheduler reduces the learning rate by 0.1\n",
    "    # if the val_loss has not decreased within the last 10 epochs.\n",
    "    scheduler = {\n",
    "        # REQUIRED: The scheduler instance\n",
    "        \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "        # The unit of the scheduler's step size, could also be 'step'.\n",
    "        # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "        # updates it after a optimizer update.\n",
    "        \"interval\": \"epoch\",\n",
    "        # How many epochs/steps should pass between calls to\n",
    "        # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "        # rate after every epoch/step.\n",
    "        \"frequency\": 1,\n",
    "        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "        \"monitor\": \"val_loss\",\n",
    "        # If set to `True`, will enforce that the value specified 'monitor'\n",
    "        # is available when the scheduler is updated, thus stopping\n",
    "        # training if not found. If set to `False`, it will only produce a warning\n",
    "        \"strict\": True,\n",
    "        # If using the `LearningRateMonitor` callback to monitor the\n",
    "        # learning rate progress, this keyword can be used to specify\n",
    "        # a custom logged name\n",
    "        \"name\": None,\n",
    "    }\n",
    "    return {\"optimizer\": optimizer, 'lr-scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Fit function deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b920c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the original ResNet Layer.\n",
    "# Find details on ResNet: https://arxiv.org/abs/1512.03385\n",
    "# Find details on Batch Normalization: https://arxiv.org/abs/1502.03167\n",
    "class MyResLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "            \n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_res = self.res_block(x)\n",
    "        x = self.proj_out(x)\n",
    "        return  F.relu(x + x_res) #x + x_res\n",
    "\n",
    "tree_model = MyCNNModel(\n",
    "    MyResLayer(3, 8, (3,3), padding='same'),\n",
    "    MyResLayer(8, 8, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(8, 16, (3,3), padding='same'),\n",
    "    MyResLayer(16, 16, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(16, 32, (3,3), padding='same'),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    MyResLayer(32, 32, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(32, ...),\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='32-true', max_epochs=1,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=50),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(tree_model, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900cc27",
   "metadata": {},
   "source": [
    "## Now, we can apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = testset[i]\n",
    "    y_hat = tree_model.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, testset.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
