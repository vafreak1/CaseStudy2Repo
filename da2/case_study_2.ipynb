{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb7d6fef",
   "metadata": {},
   "source": [
    "## First, we slice the train images into 31 x 31 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndigit(n, x):\n",
    "    x = str(x)\n",
    "    while(len(x) < n):\n",
    "        x = \"0\" + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(res, files = 20):\n",
    "    j = 0\n",
    "    path = [\"02\", \"train\"]\n",
    "    res = int((res-1)/2)\n",
    "    \n",
    "    for p in path:\n",
    "        for f in range(files):\n",
    "            image = np.load(f\"images_{p}/images/image_{ndigit(3, f)}.npy\")\n",
    "            mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "            image = np.reshape(image, (1024,1024,10))\n",
    "            mask = np.reshape(mask, (1024,1024,1))\n",
    "\n",
    "            # Add padding to every image (and mask) edge in case there are ground truths which are too close to an edge\n",
    "            padded_image = np.pad(image, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "            padded_mask = np.pad(mask, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "\n",
    "            # Extract ground truths\n",
    "            ground_truths_pos = np.array(np.where(padded_mask != 0)).T\n",
    "            \n",
    "            # Slice and save patches around each ground truth\n",
    "            for i in ground_truths_pos: \n",
    "                patch = (padded_image[i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1, :], padded_mask[i[0], i[1], 0])\n",
    "                np.save(f\"patches/train/patch_{p}_{ndigit(3, f)}_{ndigit(5, j)}.npy\", np.array(patch, dtype=\"object\"))                                 \n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31235460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of ground truths\n",
    "# pos = 0\n",
    "# for i in range(20):\n",
    "#     mask = np.load(f\"masks_02/masks/mask_{ndigit(3, i)}.npy\")\n",
    "#     ground_truths_pos = np.array(np.where(mask != 0)).T\n",
    "#     pos = pos + len(ground_truths_pos)\n",
    "# print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fce53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 15\n",
    "load_data(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ca66217",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38863"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load patches\n",
    "directory = 'patches/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "trainset0 = [np.load(file_path, allow_pickle=True) for file_path in file_paths]\n",
    "trainset = []\n",
    "for pic in trainset0:\n",
    "    trainset.append(pic)\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21d208fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[[1968, 2409, 2409, ..., 2564, 2564, 3006],\n",
       "               [3006, 3047, 3047, ..., 2631, 2631, 2165],\n",
       "               [2165, 2307, 2307, ..., 2619, 2619, 2545],\n",
       "               ...,\n",
       "               [2543, 2499, 2499, ..., 2129, 2129, 2124],\n",
       "               [2124, 2121, 2121, ..., 2239, 2239, 2223],\n",
       "               [2223, 2232, 2232, ..., 2008, 2008, 2101]],\n",
       "\n",
       "              [[2305, 2245, 2245, ..., 2123, 2123, 2160],\n",
       "               [2160, 2176, 2176, ..., 2997, 2997, 2350],\n",
       "               [2350, 2226, 2226, ..., 1983, 1983, 2088],\n",
       "               ...,\n",
       "               [2057, 2103, 2103, ..., 2527, 2527, 2737],\n",
       "               [2737, 2847, 2847, ..., 2684, 2684, 2288],\n",
       "               [2288, 2296, 2296, ..., 2282, 2282, 2629]],\n",
       "\n",
       "              [[2355, 2573, 2573, ..., 2291, 2291, 2307],\n",
       "               [2307, 2341, 2341, ..., 3254, 3254, 2674],\n",
       "               [2674, 2364, 2364, ..., 2120, 2120, 1991],\n",
       "               ...,\n",
       "               [2243, 2289, 2289, ..., 2372, 2372, 2389],\n",
       "               [2389, 2411, 2411, ..., 2579, 2579, 2571],\n",
       "               [2571, 2577, 2577, ..., 2291, 2291, 2244]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[1051, 1354, 1354, ..., 1656, 1656, 1730],\n",
       "               [1730, 1417, 1417, ..., 1212, 1212, 1897],\n",
       "               [1897, 2047, 2047, ..., 2037, 2037, 2816],\n",
       "               ...,\n",
       "               [2746, 2719, 2719, ..., 2080, 2080, 2089],\n",
       "               [2089, 2218, 2218, ..., 2527, 2527, 2678],\n",
       "               [2678, 3038, 3038, ..., 3075, 3075, 2893]],\n",
       "\n",
       "              [[1346, 1456, 1456, ..., 1211, 1211,  836],\n",
       "               [ 836,  703,  703, ..., 2286, 2286, 2122],\n",
       "               [2122, 2344, 2344, ..., 2316, 2316, 2258],\n",
       "               ...,\n",
       "               [2760, 2795, 2795, ..., 2239, 2239, 2125],\n",
       "               [2125, 2117, 2117, ..., 2415, 2415, 2554],\n",
       "               [2554, 2521, 2521, ..., 2359, 2359, 2089]],\n",
       "\n",
       "              [[1540, 1392, 1392, ..., 1377, 1377, 1990],\n",
       "               [1990, 2176, 2176, ..., 2178, 2178, 2183],\n",
       "               [2183, 2124, 2124, ..., 1911, 1911, 2042],\n",
       "               ...,\n",
       "               [2352, 2405, 2405, ..., 2879, 2879, 2748],\n",
       "               [2748, 2763, 2763, ..., 2680, 2680, 2752],\n",
       "               [2752, 2711, 2711, ..., 2710, 2710, 2684]]], dtype=int16),\n",
       "       2.5399999618530273], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed9c374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_channels(trainset, veggie, moisture):\n",
    "    #trainset[pic_no][0][h][w][channel] -> pixel value\n",
    "    print(f\"Shape vorher: Liste mit ({res},{res},10) Bildern\")\n",
    "    counter = 0\n",
    "    trainset = trainset.copy()  # Make a copy of the trainset\n",
    "\n",
    "    if veggie:\n",
    "        pic_no = 0\n",
    "        for pic in trainset:\n",
    "            counter += 1\n",
    "            pixel_values = pic[0]\n",
    "            channel8 = pixel_values[:, :, 7]\n",
    "            channel4 = pixel_values[:, :, 3]\n",
    "            channels = pic[0].shape[2]\n",
    "            width = pic[0].shape[0]\n",
    "            height = pic[0].shape[1]\n",
    "\n",
    "            vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(channel8, channel4))\n",
    "            trainset_transformed = np.concatenate((trainset[pic_no][0], vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "            trainset[pic_no] = (trainset_transformed, trainset[pic_no][1])\n",
    "            pic_no += 1\n",
    "\n",
    "        print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "\n",
    "    if moisture:\n",
    "        pic_no = 0\n",
    "        for pic in trainset:\n",
    "            pixel_values = pic[0]\n",
    "            channel8a = pixel_values[:, :, 7]\n",
    "            channel11 = pixel_values[:, :, 8]\n",
    "            channels = pic[0].shape[2]\n",
    "            width = pic[0].shape[0]\n",
    "            height = pic[0].shape[1]\n",
    "\n",
    "            moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(channel8a, channel11))\n",
    "            trainset_transformed = np.concatenate((trainset[pic_no][0], moisture_array[:, :, np.newaxis]), axis=2)\n",
    "            trainset[pic_no] = (trainset_transformed, trainset[pic_no][1])\n",
    "            pic_no += 1\n",
    "\n",
    "        print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "        \n",
    "    print(\"shape nachher\", trainset[0][0].shape)  # Print the shape of the first item\n",
    "\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d4d8018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape vorher: Liste mit (15,15,10) Bildern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damian.garrell\\AppData\\Local\\Temp\\ipykernel_8284\\950853731.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(channel8, channel4))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Vegetation (B8-B4)/(B8+B4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damian.garrell\\AppData\\Local\\Temp\\ipykernel_8284\\950853731.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(channel8a, channel11))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Moisture (B8A-B11)/(B8A+B11)\n",
      "shape nachher (15, 15, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38863"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_enriched = enrich_channels(trainset, True, True)\n",
    "len(trainset_enriched)\n",
    "#trainset[pic_no][0][h][w][channel] -> pixel value\n",
    "#trainset[pic_no][1] -> Ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "541939c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 12)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = trainset_enriched[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0a187451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, trainset, transform, augmentations):\n",
    "        self.trainset = trainset\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trainset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.trainset[index]\n",
    "\n",
    "        # apply each transformation jointly to each input\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # apply each augmentation separately to each input\n",
    "        if self.augmentations:\n",
    "            for augmentation in self.augmentations:\n",
    "                data = augmentation(data)\n",
    "\n",
    "\n",
    "        return data, target\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.ConvertImageDtype(torch.float64),\n",
    "     transforms.Lambda(lambda x : x / 3000),\n",
    "     transforms.Lambda(lambda x : torch.where(x > 1, 1, x)), # clip images between 0 and 1\n",
    "     transforms.Normalize(mean=(0.5,)*12,\n",
    "                          std=(0.5,)*12)\n",
    "     ])\n",
    "\n",
    "augmentations = [\n",
    "     transforms.RandomRotation(360),\n",
    "     transforms.RandomAffine(degrees=0, translate=(0.5,0.5)), # shift in both directions along 0.5 * height on y-axis and 0.5 * width on x-axis\n",
    "     transforms.RandomAffine(0, scale=(10,45)), # scale in range 10 <= scale <= 45\n",
    "     transforms.RandomAffine(0, shear=[10,30,10,30]), # shear on x- and y-axis between (10,30) \n",
    "     transforms.ElasticTransform(alpha=50.0, sigma=3.0) # displaces pixels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "69f0648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38863"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the custom dataset\n",
    "trainset_transformed = CustomDataset(trainset_enriched, transform=transform, augmentations=augmentations)\n",
    "\n",
    "len(trainset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e5b05618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1355,  0.5291,  0.5447,  ...,  0.1558,  0.1412,  0.1429],\n",
       "          [ 0.4109,  0.5447,  0.5447,  ...,  0.5447,  0.5447,  0.5447],\n",
       "          [ 0.2089,  0.5447,  0.5447,  ...,  0.5447,  0.5447,  0.5447],\n",
       "          ...,\n",
       "          [ 0.5447,  0.5447,  0.5447,  ...,  0.5447,  0.5447,  0.4957],\n",
       "          [ 0.2896,  0.3310,  0.4346,  ...,  0.5447,  0.5447,  0.5190],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.5447,  0.4898,  0.3490]],\n",
       " \n",
       "         [[ 0.1546,  0.6035,  0.6213,  ...,  0.1778,  0.1611,  0.1631],\n",
       "          [ 0.4688,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.6213],\n",
       "          [ 0.2383,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.6213],\n",
       "          ...,\n",
       "          [ 0.6213,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.5654],\n",
       "          [ 0.3304,  0.3776,  0.4958,  ...,  0.6213,  0.6213,  0.5920],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.6213,  0.5587,  0.3981]],\n",
       " \n",
       "         [[ 0.1546,  0.6035,  0.6213,  ...,  0.1778,  0.1611,  0.1631],\n",
       "          [ 0.4688,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.6213],\n",
       "          [ 0.2383,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.6213],\n",
       "          ...,\n",
       "          [ 0.6213,  0.6213,  0.6213,  ...,  0.6213,  0.6213,  0.5654],\n",
       "          [ 0.3304,  0.3776,  0.4958,  ...,  0.6213,  0.6213,  0.5920],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.6213,  0.5587,  0.3981]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1650,  0.6443,  0.6633,  ...,  0.1898,  0.1720,  0.1741],\n",
       "          [ 0.5005,  0.6633,  0.6633,  ...,  0.6633,  0.6633,  0.6633],\n",
       "          [ 0.2544,  0.6633,  0.6633,  ...,  0.6633,  0.6633,  0.6633],\n",
       "          ...,\n",
       "          [ 0.6633,  0.6633,  0.6633,  ...,  0.6633,  0.6633,  0.6037],\n",
       "          [ 0.3527,  0.4032,  0.5293,  ...,  0.6633,  0.6633,  0.6321],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.6633,  0.5965,  0.4251]],\n",
       " \n",
       "         [[-0.2487, -0.9713, -1.0000,  ..., -0.2861, -0.2592, -0.2624],\n",
       "          [-0.7545, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.3835, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9100],\n",
       "          [-0.5318, -0.6078, -0.7980,  ..., -1.0000, -1.0000, -0.9529],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -0.8992, -0.6408]],\n",
       " \n",
       "         [[-0.2487, -0.9713, -1.0000,  ..., -0.2861, -0.2592, -0.2624],\n",
       "          [-0.7545, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.3835, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9100],\n",
       "          [-0.5318, -0.6078, -0.7980,  ..., -1.0000, -1.0000, -0.9529],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -0.8992, -0.6408]]]),\n",
       " 2.5399999618530273)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31090 7773\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of the training set and validation set\n",
    "train_size = int(0.8 * len(trainset_transformed))\n",
    "val_size = len(trainset_transformed) - train_size\n",
    "\n",
    "# Split trainset into trainset and valset\n",
    "trainset_load, valset_load = random_split(trainset_transformed, [train_size, val_size])\n",
    "print(len(trainset_load), len(valset_load))\n",
    "\n",
    "# Create data loaders for the training set and validation set\n",
    "trainloader = DataLoader(trainset_load, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "validloader = DataLoader(valset_load, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "    def __init__(self, *layers, classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = 0.01  # Assign the learning rate here\n",
    "        self.classes = classes\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)  # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_prefix='train'):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        self.log(f\"{log_prefix}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam with Weight Decay\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        # Simplest scheduler is ReduceLROnPlateau. This scheduler reduces the learning rate by 0.1\n",
    "        # if the val_loss has not decreased within the last 10 epochs.\n",
    "        scheduler = {\n",
    "            # REQUIRED: The scheduler instance\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "            # The unit of the scheduler's step size, could also be 'step'.\n",
    "            # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "            # updates it after a optimizer update.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"val_loss\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated, thus stopping\n",
    "            # training if not found. If set to `False`, it will only produce a warning\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, 'lr-scheduler': scheduler}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "be94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements entry to SepConv2d, see Lang et al. (2019), p. 6\n",
    "class MyEntryLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.proj_out = nn.Conv2d(in_channels, out_channels[len(out_channels)-1], (1,1))\n",
    "\n",
    "        self.entry_blocks = nn.ModuleList()\n",
    "        for i in range(len(out_channels)):\n",
    "            self.entry_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels[i], (1, 1)),\n",
    "                nn.BatchNorm2d(out_channels[i]),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "            in_channels = out_channels[i]  # Update in_channels for next iteration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_entry = x\n",
    "        for i in range(len(self.out_channels)):\n",
    "            x_entry = self.entry_blocks[i](x_entry)\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "61d1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements SepConv2D\n",
    "class MySepConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "\n",
    "        self.sep_conv_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, groups=in_channels, **kwargs), # depthwise SepConv\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs), # pointwise SepConv\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_sep_conv = self.sep_conv_block(x)\n",
    "        x_sep_conv_2 = self.sep_conv_block(x_sep_conv) # performs second SepConv, see Lang et al. (2019), p. 6\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_sep_conv_2) # adds original input and sep_conv_2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "823bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = MyCNNModel(\n",
    "    MyEntryLayer(12, [128, 256]), # increase number of channels to 512\n",
    "    MySepConvLayer(256, 256, (3,3), padding='same'),\n",
    "    MySepConvLayer(256, 256, (3,3), padding='same'),\n",
    "    MySepConvLayer(256, 256, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(256, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36dec192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='64', max_epochs=1,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=50),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4795f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\damian.garrell\\AppData\\Local\\anaconda3\\envs\\venv_dl\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:357: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'lr-scheduler'}\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ layers                  │ Sequential        │  245 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ layers.0                │ MyEntryLayer      │ 38.8 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ layers.0.proj_out       │ Conv2d            │  3.3 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ layers.0.entry_blocks   │ ModuleList        │ 35.5 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ layers.1                │ MySepConvLayer    │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ layers.1.proj_out       │ Identity          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ layers.1.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ layers.2                │ MySepConvLayer    │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ layers.2.proj_out       │ Identity          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ layers.2.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ layers.3                │ MySepConvLayer    │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ layers.3.proj_out       │ Identity          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>│ layers.3.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>│ layers.4                │ AdaptiveMaxPool2d │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>│ layers.5                │ Flatten           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>│ layers.6                │ Linear            │    257 │\n",
       "└────┴─────────────────────────┴───────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ layers                  │ Sequential        │  245 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ layers.0                │ MyEntryLayer      │ 38.8 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ layers.0.proj_out       │ Conv2d            │  3.3 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ layers.0.entry_blocks   │ ModuleList        │ 35.5 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ layers.1                │ MySepConvLayer    │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ layers.1.proj_out       │ Identity          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ layers.1.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ layers.2                │ MySepConvLayer    │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ layers.2.proj_out       │ Identity          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ layers.2.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ layers.3                │ MySepConvLayer    │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ layers.3.proj_out       │ Identity          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ layers.3.sep_conv_block │ Sequential        │ 68.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ layers.4                │ AdaptiveMaxPool2d │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0m│ layers.5                │ Flatten           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0m│ layers.6                │ Linear            │    257 │\n",
       "└────┴─────────────────────────┴───────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 245 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 245 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 245 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 245 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a1124056f94f07b728e0df90fb7ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer1.fit(tree_model, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0b50a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 15, 15])\n",
      "forward: torch.Size([128, 12, 15, 15])\n",
      "Predictions: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]])\n"
     ]
    }
   ],
   "source": [
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "batch = next(iter(trainloader))\n",
    "inputs = batch[0]\n",
    "inputs = inputs.float()\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = tree_model(inputs)\n",
    "\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
