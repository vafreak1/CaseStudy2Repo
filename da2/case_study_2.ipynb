{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d6fef",
   "metadata": {},
   "source": [
    "## First, we slice the train images into 15 x 15 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to fetch and save files in load_data()\n",
    "def ndigit(n, x):\n",
    "    x = str(x)\n",
    "    while(len(x) < n):\n",
    "        x = \"0\" + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images, enrich them with moisture and vegetation index (i.e. increase channels from 10 to 12),\n",
    "# extract ground truths from the masks, pad the images to work with ground truths close to the edges,\n",
    "# slice images into 15 x 15 patches and save them.\n",
    "def load_data(res, files = 20):\n",
    "    j = 0\n",
    "    path = [\"02\", \"train\"]\n",
    "    res = int((res-1)/2)\n",
    "    nan_values = 0\n",
    "\n",
    "    # Load images and masks\n",
    "    for p in path:\n",
    "        for f in range(files):\n",
    "            image = np.load(f\"images_{p}/images/image_{ndigit(3, f)}.npy\")\n",
    "            mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "            \n",
    "            # In anticipation of toTensor() in transforms later which expects an array of H x W x C and converts it into C x H x W.\n",
    "            image = np.transpose(image, (1,2,0))\n",
    "            mask = np.transpose(mask, (1,2,0))\n",
    "            \n",
    "            nan_values_before = (np.count_nonzero(np.isnan(image)))\n",
    "            \n",
    "            # Extract spectral bands for calculating vegetation index\n",
    "            channel8 = image[:, :, 6]\n",
    "            channel4 = image[:, :, 2]\n",
    "\n",
    "            # Calculate the vegetation index with small epsilon in order to prevent dividing through zero (which results in NaN values)\n",
    "            vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(np.add(channel8, channel4), 1e-6))\n",
    "            \n",
    "            nan_values_vegetation = (np.count_nonzero(np.isnan(vegetation_array)))\n",
    "            \n",
    "            if(nan_values_vegetation > 0): \n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy had\", nan_values_before, \"before vegetation index\")\n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy has\", nan_values_vegetation, \"nan_values after adding vegetation\")\n",
    "            \n",
    "            \n",
    "            vegetation_array = np.nan_to_num(vegetation_array, nan=0.0)\n",
    "\n",
    "            # Add vegetation index to the image as eleventh channel\n",
    "            image_veg = np.concatenate((image, vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "\n",
    "            nan_values_before = 0\n",
    "            \n",
    "            # Extract spectral bands for calculating moisture index\n",
    "            channel8a = image[:, :, 7]\n",
    "            channel11 = image[:, :, 8]\n",
    "\n",
    "            # Calculate the moisture index with small epsilon in order to prevent dividing through zero\n",
    "            moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(np.add(channel8a, channel11), 1e-6))\n",
    "            \n",
    "            nan_values_moisture = (np.count_nonzero(np.isnan(moisture_array)))\n",
    "            \n",
    "            if(nan_values_moisture > 0): \n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy had\", nan_values_before, \"before moisture index\")\n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy has\", nan_values_moisture, \"nan_values after adding moisture\")\n",
    "            \n",
    "            # Add moisture index to the image as twelfth channel\n",
    "            image_veg_mois = np.concatenate((image_veg,moisture_array[:,:, np.newaxis]), axis = 2)\n",
    "\n",
    "            nan_values_pic = np.count_nonzero(np.isnan(image_veg_mois))\n",
    "            nan_values += nan_values_pic\n",
    "\n",
    "            # Add padding to every image and mask edge in case there are ground truths which are too close to an edge\n",
    "            padded_image = np.pad(image_veg_mois, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "            padded_mask = np.pad(mask, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "\n",
    "            # Extract ground truths\n",
    "            ground_truths_pos = np.array(np.where(padded_mask != 0)).T\n",
    "            \n",
    "            # Slice and save patches around each ground truth\n",
    "            for i in ground_truths_pos: \n",
    "                patch = (padded_image[i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1, :], padded_mask[i[0], i[1], 0])\n",
    "                np.save(f\"patches/train/patch_{p}_{ndigit(3, f)}_{ndigit(5, j)}.npy\", np.array(patch, dtype=\"object\"))                                 \n",
    "                j += 1\n",
    "    print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "    print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "    print(\"Patched the pictures\")\n",
    "    print(\"NaN values:\", nan_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31235460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of ground truths\n",
    "# num = 0\n",
    "# path = [\"02\", \"train\"]\n",
    "# for p in path:\n",
    "#     for f in range(20):\n",
    "#         mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "#         ground_truths_pos = np.array(np.where(mask != 0)).T\n",
    "#         num = num + len(ground_truths_pos)\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fce53c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = 15\n",
    "#load_data(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patches\n",
    "directory = 'patches/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "dataset = [np.load(file_path, allow_pickle=True) for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541939c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset contains 38863 samples where each input shape is (15, 15, 12) and target shape is ().\n"
     ]
    }
   ],
   "source": [
    "# Check some metrics on the dataset\n",
    "l_t = len(dataset)\n",
    "X,y = dataset[0]\n",
    "X_s = X.shape\n",
    "y_s = y.shape\n",
    "print(f\"Trainset contains {l_t} samples where each input shape is {X_s} and target shape is {y_s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56d9a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[[7.10000000e+02, 9.44000000e+02, 1.12600000e+03, ...,\n",
       "                1.73300000e+03, 3.53802009e-01, 5.10183505e-02],\n",
       "               [7.73000000e+02, 9.66000000e+02, 1.10600000e+03, ...,\n",
       "                1.73300000e+03, 3.61616162e-01, 5.10183505e-02],\n",
       "               [5.86000000e+02, 8.70000000e+02, 9.98000000e+02, ...,\n",
       "                1.69800000e+03, 4.78578892e-01, 1.13785970e-01],\n",
       "               ...,\n",
       "               [5.26000000e+02, 6.14000000e+02, 6.02000000e+02, ...,\n",
       "                1.30600000e+03, 4.68901632e-01, 4.63042853e-02],\n",
       "               [4.82000000e+02, 5.94000000e+02, 6.07000000e+02, ...,\n",
       "                1.30600000e+03, 4.60444444e-01, 4.63042853e-02],\n",
       "               [4.96000000e+02, 6.18000000e+02, 6.02000000e+02, ...,\n",
       "                1.19300000e+03, 4.76976542e-01, 5.99423631e-02]],\n",
       "\n",
       "              [[7.04000000e+02, 9.52000000e+02, 1.16000000e+03, ...,\n",
       "                1.73300000e+03, 3.66639367e-01, 5.10183505e-02],\n",
       "               [7.58000000e+02, 9.60000000e+02, 1.11000000e+03, ...,\n",
       "                1.73300000e+03, 3.35528285e-01, 5.10183505e-02],\n",
       "               [6.24000000e+02, 9.88000000e+02, 1.11800000e+03, ...,\n",
       "                1.69800000e+03, 4.29446287e-01, 1.13785970e-01],\n",
       "               ...,\n",
       "               [4.92000000e+02, 6.04000000e+02, 5.96000000e+02, ...,\n",
       "                1.30600000e+03, 4.87972508e-01, 4.63042853e-02],\n",
       "               [4.76000000e+02, 5.83000000e+02, 6.00000000e+02, ...,\n",
       "                1.30600000e+03, 4.76668120e-01, 4.63042853e-02],\n",
       "               [4.95000000e+02, 5.94000000e+02, 6.12000000e+02, ...,\n",
       "                1.19300000e+03, 4.72413793e-01, 5.99423631e-02]],\n",
       "\n",
       "              [[7.19000000e+02, 9.54000000e+02, 1.13600000e+03, ...,\n",
       "                1.85500000e+03, 4.01159726e-01, 4.05083399e-02],\n",
       "               [7.30000000e+02, 9.52000000e+02, 1.05800000e+03, ...,\n",
       "                1.85500000e+03, 3.59370269e-01, 4.05083399e-02],\n",
       "               [6.44000000e+02, 9.86000000e+02, 1.15600000e+03, ...,\n",
       "                1.84800000e+03, 4.00881057e-01, 7.18855852e-02],\n",
       "               ...,\n",
       "               [4.78000000e+02, 6.04000000e+02, 5.96000000e+02, ...,\n",
       "                1.30600000e+03, 4.91901108e-01, 4.29343190e-02],\n",
       "               [4.79000000e+02, 5.56000000e+02, 6.00000000e+02, ...,\n",
       "                1.30600000e+03, 4.81865285e-01, 4.29343190e-02],\n",
       "               [4.90000000e+02, 5.84000000e+02, 5.96000000e+02, ...,\n",
       "                1.19200000e+03, 4.84205971e-01, 4.73149492e-02]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[7.20000000e+02, 1.05000000e+03, 1.40000000e+03, ...,\n",
       "                1.88700000e+03, 3.28053756e-01, 5.02987088e-02],\n",
       "               [7.40000000e+02, 9.28000000e+02, 1.10000000e+03, ...,\n",
       "                1.88700000e+03, 3.20987654e-01, 5.02987088e-02],\n",
       "               [7.19000000e+02, 9.50000000e+02, 1.08000000e+03, ...,\n",
       "                1.85500000e+03, 3.82151030e-01, 6.56095144e-02],\n",
       "               ...,\n",
       "               [7.14000000e+02, 1.05000000e+03, 1.46600000e+03, ...,\n",
       "                1.83300000e+03, 3.77494692e-01, 1.53292362e-01],\n",
       "               [6.50000000e+02, 1.02800000e+03, 1.32200000e+03, ...,\n",
       "                1.83300000e+03, 3.81230985e-01, 1.53292362e-01],\n",
       "               [6.37000000e+02, 1.01200000e+03, 1.26800000e+03, ...,\n",
       "                2.08200000e+03, 3.83568303e-01, 8.44494047e-02]],\n",
       "\n",
       "              [[6.68000000e+02, 9.51000000e+02, 1.38200000e+03, ...,\n",
       "                1.88700000e+03, 3.89306231e-01, 5.02987088e-02],\n",
       "               [7.64000000e+02, 9.24000000e+02, 1.13600000e+03, ...,\n",
       "                1.88700000e+03, 3.32745962e-01, 5.02987088e-02],\n",
       "               [7.01000000e+02, 9.52000000e+02, 1.06200000e+03, ...,\n",
       "                1.85500000e+03, 3.94871795e-01, 6.56095144e-02],\n",
       "               ...,\n",
       "               [7.42000000e+02, 1.09400000e+03, 1.54800000e+03, ...,\n",
       "                1.83300000e+03, 3.84615385e-01, 1.53292362e-01],\n",
       "               [7.20000000e+02, 1.07600000e+03, 1.49000000e+03, ...,\n",
       "                1.83300000e+03, 3.74081075e-01, 1.53292362e-01],\n",
       "               [6.46000000e+02, 1.02800000e+03, 1.38800000e+03, ...,\n",
       "                2.08200000e+03, 3.70378771e-01, 8.44494047e-02]],\n",
       "\n",
       "              [[7.20000000e+02, 9.98000000e+02, 1.38200000e+03, ...,\n",
       "                1.84800000e+03, 4.07121407e-01, 9.28462709e-02],\n",
       "               [7.58000000e+02, 9.84000000e+02, 1.11200000e+03, ...,\n",
       "                1.84800000e+03, 3.62019506e-01, 9.28462709e-02],\n",
       "               [7.04000000e+02, 9.51000000e+02, 1.06400000e+03, ...,\n",
       "                1.79200000e+03, 3.84259259e-01, 4.31556337e-02],\n",
       "               ...,\n",
       "               [7.30000000e+02, 1.09000000e+03, 1.51000000e+03, ...,\n",
       "                1.86700000e+03, 3.79876797e-01, 1.16841544e-01],\n",
       "               [7.30000000e+02, 1.10200000e+03, 1.55800000e+03, ...,\n",
       "                1.86700000e+03, 3.73416449e-01, 1.16841544e-01],\n",
       "               [6.79000000e+02, 1.06800000e+03, 1.48800000e+03, ...,\n",
       "                2.00700000e+03, 3.74527112e-01, 1.44255989e-01]]])  ,\n",
       "       2.5399999618530273], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6705133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31090 7773\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of the training set and validation set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split trainset into trainset and valset\n",
    "trainset, valset = random_split(dataset, [train_size, val_size])\n",
    "print(len(trainset), len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80a20b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31090, 15, 15, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacks data for all train images in one array\n",
    "inputs = np.stack([data[0] for data in trainset], axis=0)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc31cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n",
      "[4.99621724e+02 7.37692006e+02 7.11432591e+02 1.18871994e+03\n",
      " 2.47144648e+03 2.93903173e+03 3.07567553e+03 3.20778523e+03\n",
      " 2.04999702e+03 1.24576666e+03 6.29505878e-01 2.24810481e-01] [2.81440099e+02 3.47990360e+02 5.03348864e+02 5.03846522e+02\n",
      " 6.36426717e+02 7.78754342e+02 8.43533804e+02 8.32242617e+02\n",
      " 6.79728210e+02 5.87340485e+02 2.13783702e-01 1.43432713e-01]\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and standard deviation for each channel from all pictures along heigth and width\n",
    "channel_means = np.mean(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_stds = np.std(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_meds = np.median(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_mins = np.min(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_maxs = np.max(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_10_quants = np.quantile(inputs, 0.1, axis=(0, 1, 2), keepdims=False)\n",
    "channel_90_quants = np.quantile(inputs, 0.9, axis=(0, 1, 2), keepdims=False)\n",
    "\n",
    "print(len(channel_means), len(channel_stds))\n",
    "print(channel_means, channel_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784d0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 1\n",
      "mean =  499.62172431292663\n",
      "std =  281.44009926313737\n",
      "median =  430.0\n",
      "min =  0.0\n",
      "max =  10912.0\n",
      "10 percent quantile =  236.0\n",
      "90 percent quantile =  866.0\n",
      "\n",
      "Channel 2\n",
      "mean =  737.6920064329366\n",
      "std =  347.9903600970017\n",
      "median =  694.0\n",
      "min =  0.0\n",
      "max =  11752.0\n",
      "10 percent quantile =  381.0\n",
      "90 percent quantile =  1212.0\n",
      "\n",
      "Channel 3\n",
      "mean =  711.4325905435832\n",
      "std =  503.34886399265343\n",
      "median =  550.0\n",
      "min =  0.0\n",
      "max =  13104.0\n",
      "10 percent quantile =  228.0\n",
      "90 percent quantile =  1464.0\n",
      "\n",
      "Channel 4\n",
      "mean =  1188.719939101533\n",
      "std =  503.84652156095876\n",
      "median =  1157.0\n",
      "min =  0.0\n",
      "max =  10560.0\n",
      "10 percent quantile =  639.0\n",
      "90 percent quantile =  1894.0\n",
      "\n",
      "Channel 5\n",
      "mean =  2471.44647996855\n",
      "std =  636.4267173367139\n",
      "median =  2543.0\n",
      "min =  0.0\n",
      "max =  9914.0\n",
      "10 percent quantile =  1681.0\n",
      "90 percent quantile =  3177.0\n",
      "\n",
      "Channel 6\n",
      "mean =  2939.0317329616523\n",
      "std =  778.7543423428289\n",
      "median =  2988.0\n",
      "min =  0.0\n",
      "max =  10352.0\n",
      "10 percent quantile =  1977.0\n",
      "90 percent quantile =  3857.0\n",
      "\n",
      "Channel 7\n",
      "mean =  3075.675531539259\n",
      "std =  843.5338043451787\n",
      "median =  3136.0\n",
      "min =  0.0\n",
      "max =  12784.0\n",
      "10 percent quantile =  2042.0\n",
      "90 percent quantile =  4079.0\n",
      "\n",
      "Channel 8\n",
      "mean =  3207.7852345520173\n",
      "std =  832.2426169783005\n",
      "median =  3279.0\n",
      "min =  0.0\n",
      "max =  9803.0\n",
      "10 percent quantile =  2172.0\n",
      "90 percent quantile =  4181.0\n",
      "\n",
      "Channel 9\n",
      "mean =  2049.997015260355\n",
      "std =  679.7282096707443\n",
      "median =  2069.0\n",
      "min =  0.0\n",
      "max =  10745.0\n",
      "10 percent quantile =  1189.0\n",
      "90 percent quantile =  2900.0\n",
      "\n",
      "Channel 10\n",
      "mean =  1245.7666583753262\n",
      "std =  587.3404852919342\n",
      "median =  1174.0\n",
      "min =  0.0\n",
      "max =  12144.0\n",
      "10 percent quantile =  585.0\n",
      "90 percent quantile =  2025.0\n",
      "\n",
      "Channel 11\n",
      "mean =  0.6295058783813894\n",
      "std =  0.21378370155274568\n",
      "median =  0.6906444903573205\n",
      "min =  -0.7114228442656857\n",
      "max =  0.9374867104109107\n",
      "10 percent quantile =  0.328438948931909\n",
      "90 percent quantile =  0.8632202474789391\n",
      "\n",
      "Channel 12\n",
      "mean =  0.22481048111445875\n",
      "std =  0.14343271276540692\n",
      "median =  0.2374318594071981\n",
      "min =  -0.5327607360655802\n",
      "max =  0.6200807263824216\n",
      "10 percent quantile =  0.023872679041574908\n",
      "90 percent quantile =  0.3992793972487259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(inputs.shape[3]):\n",
    "  print(f\"Channel {i+1}\")\n",
    "  print(f\"mean = {channel_means[i]}\")\n",
    "  print(f\"std = {channel_stds[i]}\")\n",
    "  print(f\"median = {channel_meds[i]}\")\n",
    "  print(f\"min = {channel_mins[i]}\")\n",
    "  print(f\"max = {channel_maxs[i]}\")\n",
    "  print(f\"10 percent quantile = {channel_10_quants[i]}\")\n",
    "  print(f\"90 percent quantile = {channel_90_quants[i]}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a187451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class in order to transform dataset and apply data augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, augmentations):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.augment = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "           data = self.augment(data)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c054c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom rotation transformation from the documentation in order to rotate at given angles,\n",
    "# not select from range of angles.\n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, image):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(image, angle)\n",
    "\n",
    "# Custom elastic transformation which adds randomness. Originally, transforms.ElasticTransform transforms\n",
    "# every image, but now only at given probability.\n",
    "class RandomElasticTransform:\n",
    "    def __init__(self, probability, alpha, sigma):\n",
    "        self.probability = probability\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if np.random.rand() < self.probability:\n",
    "          elastic_transformer = transforms.ElasticTransform(self.alpha, self.sigma)\n",
    "          return elastic_transformer(image)\n",
    "        else:\n",
    "          return image\n",
    "\n",
    "# Custom normalization class. Only the first 10 channels need to be normalized, the last two are already in normalized form\n",
    "# since their creation. The regular transforms.Normalize will throw an error, however, if less means/stds are passed than\n",
    "# there are channels.\n",
    "class MyNormalization:\n",
    "    def __init__(self, means, stds):\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        self.channels = [i for i in range(len(means))] # Channels to normalize\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i in self.channels:\n",
    "            image[i, :, :] = (image[i, :, :] - self.means[i]) / self.stds[i]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1841b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), # if input is 3D array then toTensor() switches dimensions from H x W x C to C x H x W\n",
    "     transforms.ConvertImageDtype(torch.float64),\n",
    "     #transforms.Lambda(lambda x : x / 3000),\n",
    "     #transforms.Lambda(lambda x : torch.where(x > 1, 1, x)), # fix pixel values between 0 and 1\n",
    "     MyNormalization(means=channel_means[0:10], # applies normalization with means and stds of trainset\n",
    "                          stds=channel_stds[0:10])\n",
    "     ])\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [MyRotationTransform(angles=[0, 90, 180, 270, 0]),\n",
    "     transforms.RandomAffine(degrees=0, translate=(0.2,0.2)), # shift in both directions along 0.5 * height on y-axis and 0.5 * width on x-axis\n",
    "                                                                                 # scale in range 0.25 <= scale <= 0.75\n",
    "     transforms.ElasticTransform(alpha=5.0, sigma=0.5), # displaces pixels\n",
    "     transforms.RandomHorizontalFlip(), # default p = 0.5\n",
    "     transforms.RandomVerticalFlip()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f0648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom trainset\n",
    "trainset_transformed = CustomDataset(trainset, transform=transform, augmentations=None)\n",
    "valset_transformed = CustomDataset(valset, transform=transform, augmentations=None)\n",
    "#trainset_transformed[0][0]\n",
    "#on the fly augmentation during training, hence no additional pictures in trainset\n",
    "#len(trainset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4efa000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8230, -0.8727, -0.6062,  ..., -0.2083,  0.2039,  0.6231],\n",
       "          [-0.7875, -0.9083, -0.7555,  ...,  0.2501,  0.4988,  0.6054],\n",
       "          [-0.7164, -0.8656, -0.7875,  ...,  0.3424,  0.4348,  0.5592],\n",
       "          ...,\n",
       "          [ 1.0673,  0.9749,  0.8115,  ...,  0.4633,  0.4704,  0.3638],\n",
       "          [ 1.0175,  0.9003,  0.8008,  ...,  0.3780,  0.4597,  0.3211],\n",
       "          [ 1.0175,  0.9287,  0.8186,  ...,  0.3034,  0.4277,  0.4029]],\n",
       " \n",
       "         [[-0.4963, -0.6256, -0.5250,  ...,  0.1791,  0.5957,  1.1934],\n",
       "          [-0.4934, -0.7003, -0.6974,  ...,  0.9032,  1.2682,  1.3199],\n",
       "          [-0.4158, -0.6572, -0.6486,  ...,  0.9865,  1.0584,  1.2480],\n",
       "          ...,\n",
       "          [ 1.6992,  1.5239,  1.5728,  ...,  1.3860,  1.3716,  1.0957],\n",
       "          [ 1.5613,  1.5182,  1.6187,  ...,  1.1791,  1.1791,  1.0067],\n",
       "          [ 1.3630,  1.6791,  1.6676,  ...,  1.0153,  1.1274,  1.0670]],\n",
       " \n",
       "         [[-0.7339, -0.8075, -0.5015,  ..., -0.0108,  0.5435,  0.8256],\n",
       "          [-0.6585, -0.8134, -0.6088,  ...,  0.9170,  1.1872,  1.4355],\n",
       "          [-0.5214, -0.7399, -0.6128,  ...,  1.0362,  1.1455,  1.3402],\n",
       "          ...,\n",
       "          [ 1.9004,  1.2786,  1.1991,  ...,  1.1534,  1.1196,  1.0501],\n",
       "          [ 1.6183,  1.0739,  1.3819,  ...,  1.0978,  1.1534,  0.9845],\n",
       "          [ 1.2686,  1.2110,  1.5309,  ...,  1.0461,  1.1455,  0.9826]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4934, -0.4559, -0.4559,  ...,  0.0379,  0.3988,  0.3988],\n",
       "          [-0.2295, -0.4389, -0.4389,  ...,  0.6661,  0.9947,  0.9947],\n",
       "          [-0.2295, -0.4389, -0.4389,  ...,  0.6661,  0.9947,  0.9947],\n",
       "          ...,\n",
       "          [ 1.1701,  1.3642,  1.3642,  ...,  0.1877,  0.1911,  0.1911],\n",
       "          [ 1.1513,  0.9300,  0.9300,  ...,  0.1826,  0.2030,  0.2030],\n",
       "          [ 1.1513,  0.9300,  0.9300,  ...,  0.1826,  0.2030,  0.2030]],\n",
       " \n",
       "         [[ 0.8336,  0.8406,  0.7077,  ...,  0.6601,  0.5647,  0.5203],\n",
       "          [ 0.7879,  0.8356,  0.7325,  ...,  0.4782,  0.4344,  0.3886],\n",
       "          [ 0.7165,  0.8040,  0.7292,  ...,  0.4483,  0.4290,  0.3922],\n",
       "          ...,\n",
       "          [ 0.3008,  0.4070,  0.4522,  ...,  0.4535,  0.4580,  0.4657],\n",
       "          [ 0.3456,  0.4521,  0.4451,  ...,  0.4633,  0.4567,  0.4785],\n",
       "          [ 0.4100,  0.4465,  0.4363,  ...,  0.4716,  0.4588,  0.4831]],\n",
       " \n",
       "         [[ 0.3122,  0.2796,  0.2796,  ...,  0.2100,  0.2112,  0.2112],\n",
       "          [ 0.2659,  0.2789,  0.2789,  ...,  0.1642,  0.1384,  0.1384],\n",
       "          [ 0.2659,  0.2789,  0.2789,  ...,  0.1642,  0.1384,  0.1384],\n",
       "          ...,\n",
       "          [ 0.0918,  0.1229,  0.1229,  ...,  0.2478,  0.2441,  0.2441],\n",
       "          [ 0.1114,  0.1915,  0.1915,  ...,  0.2544,  0.2463,  0.2463],\n",
       "          [ 0.1114,  0.1915,  0.1915,  ...,  0.2544,  0.2463,  0.2463]]],\n",
       "        dtype=torch.float64),\n",
       " 6.809999942779541)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for transformed training set and validation set\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(trainset_transformed, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validloader = DataLoader(valset_transformed, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit = 0\n",
    "#for batch in trainloader:\n",
    "\n",
    "    #while(limit <1):\n",
    "        #print(batch)\n",
    "    #    print(x.shape)\n",
    "    #    print(x.size)\n",
    "    #    print(y.shape)\n",
    "    #    print(y)\n",
    "        #limit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "    def __init__(self, *layers, classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = 0.01  # Assign the learning rate here\n",
    "        self.classes = classes\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)  # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_prefix='train'):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        y_hat = y_hat.flatten()\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(y_hat, y)\n",
    "        self.log(f\"{log_prefix}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam with Weight Decay\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        # ReduceLROnPlateau reduces the learning rate by 0.1 if the val_loss has not decreased within the last 10 epochs.\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "            # 'step' updates the scheduler after every step (alternative: 'epoch').\n",
    "            \"interval\": \"epoch\",\n",
    "            # Updates the learning rate after every step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to monitor for scheduler\n",
    "            \"monitor\": \"valid_loss\",\n",
    "            # Enforce that the value specified 'monitor' is available when the scheduler is updated, \n",
    "            # thus stopping training if not found.\n",
    "            \"strict\": True,\n",
    "            # No custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements entry to SepConv2d, see Lang et al. (2019), p. 6\n",
    "class MyEntryLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.proj_out = nn.Conv2d(in_channels, out_channels[len(out_channels)-1], (1,1))\n",
    "\n",
    "        self.entry_blocks = nn.ModuleList()\n",
    "        for i in range(len(out_channels)):\n",
    "            self.entry_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels[i], (1, 1)),\n",
    "                nn.BatchNorm2d(out_channels[i]),\n",
    "                nn.ReLU(inplace = True)\n",
    "            ))\n",
    "            in_channels = out_channels[i]  # Update in_channels for next iteration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_entry = x\n",
    "        for i in range(len(self.out_channels)):\n",
    "            x_entry = self.entry_blocks[i](x_entry)\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements SepConv2D\n",
    "class MySepConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "\n",
    "        self.sep_conv_block = nn.Sequential(\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, groups=in_channels, **kwargs), # depthwise SepConv\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs), # pointwise SepConv\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_sep_conv = self.sep_conv_block(x)\n",
    "        x_sep_conv_2 = self.sep_conv_block(x_sep_conv) # performs second SepConv, see Lang et al. (2019), p. 6\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_sep_conv_2) # adds original input and sep_conv_2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = MyCNNModel(\n",
    "    MyEntryLayer(12, [32, 64, 128]), # increase number of channels to 128\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(128, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='64', max_epochs=15,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=1),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(tree_model, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b50a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "batch = next(iter(trainloader))\n",
    "inputs = batch[0]\n",
    "inputs = inputs.float()\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = tree_model(inputs).flatten()\n",
    "\n",
    "true_heights = batch[1]\n",
    "print(\"Targets:\", batch[1])\n",
    "print(\"Target shape:\", batch[1].shape)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Prediction shape:\", predictions.shape)\n",
    "# expected batch size number of predictions for height !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "for batch in validloader:\n",
    "    max_ground = max(batch[1])\n",
    "    if(max_ground > max_val):\n",
    "        max_val = max_ground\n",
    "print(\"Max ground truth in validation set is: \", max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "\n",
    "num_classes = 10  # Number of size classes\n",
    "class_intervals = 4  # Interval between size classes\n",
    "# 10 classes of each 4 m because maximum height in trainset is 39.6 (see above)\n",
    "class_thresholds = [i * class_intervals for i in range(1, num_classes+1)]\n",
    "\n",
    "mse_total = [0.0] * num_classes\n",
    "mae_total = [0.0] * num_classes\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "true_heights_total = 0.0\n",
    "predictions_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(validloader, desc=\"Evaluation\")\n",
    "    for batch in progress_bar:\n",
    "        inputs, true_heights = batch[0].float(), batch[1].float()\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        predictions = tree_model(inputs)\n",
    "        true_heights = true_heights.view(-1, 1)\n",
    "\n",
    "        mse = F.mse_loss(predictions, true_heights, reduction='none').squeeze()\n",
    "        mae = F.l1_loss(predictions, true_heights, reduction='none').squeeze()\n",
    "\n",
    "        true_heights_total += true_heights.sum().item()\n",
    "        predictions_total += predictions.sum().item()\n",
    "\n",
    "        for i, threshold in enumerate(class_thresholds):\n",
    "            indices = (true_heights <= threshold).squeeze(1)\n",
    "            mse_total[i] += mse[indices].sum().item()\n",
    "            mae_total[i] += mae[indices].sum().item()\n",
    "            class_counts[i] += indices.sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({'Total MSE': mse_total[0] / class_counts[0], 'Total MAE': mae_total[0] / class_counts[0]})\n",
    "\n",
    "mse_class_avg = [mse_total[i] / class_counts[i] if class_counts[i] != 0 else 0.0 for i in range(num_classes)]\n",
    "mae_class_avg = [mae_total[i] / class_counts[i] if class_counts[i] != 0 else 0.0 for i in range(num_classes)]\n",
    "average_true_height = true_heights_total / len(validloader.dataset)\n",
    "average_prediction = predictions_total / len(validloader.dataset)\n",
    "# Calculate overall MSE and MAE\n",
    "overall_mse = sum(mse_total) / sum(class_counts) if sum(class_counts) != 0 else 0.0\n",
    "overall_mae = sum(mae_total) / sum(class_counts) if sum(class_counts) != 0 else 0.0\n",
    "\n",
    "\n",
    "# Print the evaluation metrics for each size class\n",
    "for i, threshold in enumerate(class_thresholds):\n",
    "    print(f\"Size Class {i+1}:\")\n",
    "    print(f\"MSE: {mse_class_avg[i]}\")\n",
    "    print(f\"MAE: {mae_class_avg[i]}\")\n",
    "\n",
    "# Print the overall evaluation metrics\n",
    "print(\"Average True Height:\", average_true_height)\n",
    "print(\"Average Prediction:\", average_prediction)\n",
    "print(\"Average MSE: \", overall_mse )\n",
    "print(\"Average MAE:\", overall_mae )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aff022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 15\n",
    "res = int((res-1)/2)\n",
    "nan_values = 0\n",
    "for i in range(10):\n",
    "    image = np.load(f\"test_images/image_00{i}.npy\")\n",
    "    image = np.transpose(image, (1,2,0))\n",
    "    \n",
    "    \n",
    "    nan_values_before = (np.count_nonzero(np.isnan(image)))\n",
    "            \n",
    "    channel8 = image[:, :, 6]\n",
    "    channel4 = image[:, :, 2]\n",
    "    channels = image.shape\n",
    "    width = image[0].shape[0]\n",
    "    height = image[0].shape[1]\n",
    "\n",
    "    # add the vegetation array \n",
    "    vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(np.add(channel8, channel4), 1e-6))\n",
    "            \n",
    "    nan_values_vegetation = (np.count_nonzero(np.isnan(vegetation_array)))\n",
    "            \n",
    "    if(nan_values_vegetation > 0): \n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy had\", nan_values_before, \"before vegetation index\")\n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy has\", nan_values_vegetation, \"nan_values after adding vegetation\")\n",
    "            \n",
    "            \n",
    "    vegetation_array = np.nan_to_num(vegetation_array, nan=0.0)\n",
    "    image_transformed = np.concatenate((image, vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "\n",
    "            \n",
    "    image = image_transformed\n",
    "    nan_values_before = 0\n",
    "    # add moisture index\n",
    "    channel8a = image[:, :, 7]\n",
    "    channel11 = image[:, :, 8]\n",
    "    moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(np.add(channel8a, channel11), 1e-6))\n",
    "            \n",
    "    nan_values_moisture = (np.count_nonzero(np.isnan(moisture_array)))\n",
    "            \n",
    "    if(nan_values_moisture > 0): \n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy had\", nan_values_before, \"before moisture index\")\n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy has\", nan_values_moisture, \"nan_values after adding moisture\")\n",
    "            \n",
    "            \n",
    "    image_transformed = np.concatenate((image,moisture_array[:,:, np.newaxis]), axis = 2)\n",
    "    image = image_transformed\n",
    "\n",
    "    nan_values_pic = np.count_nonzero(np.isnan(image))\n",
    "    nan_values += nan_values_pic\n",
    "    \n",
    "    # Add padding to every image edge in case there are ground truths which are too close to an edge\n",
    "    padded_image = np.pad(image, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = torch.load(\"Architecture2_0.03_10_NoAug_Model.zip\")\n",
    "    model.eval()\n",
    "    pred = np.zeros((1024, 1024))\n",
    "    for p in range(res+1, 1024+res+1):\n",
    "        for q in range(res+1, 1024+res+1):\n",
    "            patch = padded_image[p-res : p+res+1, q-res : q+res+1, :]\n",
    "            patch = torch.from_numpy(patch).float()\n",
    "            patch = torch.unsqueeze(patch, 0).permute(0,3,1,2)\n",
    "            pred[p-(res+1), q-(res+1)] = model.predict(patch)\n",
    "            \n",
    "    \n",
    "    np.save(f\"test_images/prediction_00{i}.npy\", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
