{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d6fef",
   "metadata": {},
   "source": [
    "## First, we slice the train images into 15 x 15 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to fetch and save files in load_data()\n",
    "def ndigit(n, x):\n",
    "    x = str(x)\n",
    "    while(len(x) < n):\n",
    "        x = \"0\" + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images, enrich them with moisture and vegetation index (i.e. increase channels from 10 to 12),\n",
    "# extract ground truths from the masks, pad the images to work with ground truths close to the edges,\n",
    "# slice images into 15 x 15 patches and save them.\n",
    "def load_data(res, files = 20):\n",
    "    j = 0\n",
    "    path = [\"02\", \"train\"]\n",
    "    res = int((res-1)/2)\n",
    "    nan_values = 0\n",
    "\n",
    "    # Load images and masks\n",
    "    for p in path:\n",
    "        for f in range(files):\n",
    "            image = np.load(f\"images_{p}/images/image_{ndigit(3, f)}.npy\")\n",
    "            mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "            \n",
    "            # In anticipation of toTensor() in transforms later which expects an array of H x W x C and converts it into C x H x W.\n",
    "            image = np.transpose(image, (1,2,0))\n",
    "            mask = np.transpose(mask, (1,2,0))\n",
    "            \n",
    "            nan_values_before = (np.count_nonzero(np.isnan(image)))\n",
    "            \n",
    "            # Extract spectral bands for calculating vegetation index\n",
    "            channel8 = image[:, :, 6]\n",
    "            channel4 = image[:, :, 2]\n",
    "\n",
    "            # Calculate the vegetation index with small epsilon in order to prevent dividing through zero (which results in NaN values)\n",
    "            vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(np.add(channel8, channel4), 1e-6))\n",
    "            \n",
    "            nan_values_vegetation = (np.count_nonzero(np.isnan(vegetation_array)))\n",
    "            \n",
    "            if(nan_values_vegetation > 0): \n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy had\", nan_values_before, \"before vegetation index\")\n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy has\", nan_values_vegetation, \"nan_values after adding vegetation\")\n",
    "            \n",
    "            \n",
    "            vegetation_array = np.nan_to_num(vegetation_array, nan=0.0)\n",
    "\n",
    "            # Add vegetation index to the image as eleventh channel\n",
    "            image_veg = np.concatenate((image, vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "\n",
    "            nan_values_before = 0\n",
    "            \n",
    "            # Extract spectral bands for calculating moisture index\n",
    "            channel8a = image[:, :, 7]\n",
    "            channel11 = image[:, :, 8]\n",
    "\n",
    "            # Calculate the moisture index with small epsilon in order to prevent dividing through zero\n",
    "            moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(np.add(channel8a, channel11), 1e-6))\n",
    "            \n",
    "            nan_values_moisture = (np.count_nonzero(np.isnan(moisture_array)))\n",
    "            \n",
    "            if(nan_values_moisture > 0): \n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy had\", nan_values_before, \"before moisture index\")\n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy has\", nan_values_moisture, \"nan_values after adding moisture\")\n",
    "            \n",
    "            # Add moisture index to the image as twelfth channel\n",
    "            image_veg_mois = np.concatenate((image_veg,moisture_array[:,:, np.newaxis]), axis = 2)\n",
    "\n",
    "            nan_values_pic = np.count_nonzero(np.isnan(image_veg_mois))\n",
    "            nan_values += nan_values_pic\n",
    "\n",
    "            # Add padding to every image and mask edge in case there are ground truths which are too close to an edge\n",
    "            padded_image = np.pad(image_veg_mois, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "            padded_mask = np.pad(mask, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "\n",
    "            # Extract ground truths\n",
    "            ground_truths_pos = np.array(np.where(padded_mask != 0)).T\n",
    "            \n",
    "            # Slice and save patches around each ground truth\n",
    "            for i in ground_truths_pos: \n",
    "                patch = (padded_image[i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1, :], padded_mask[i[0], i[1], 0])\n",
    "                np.save(f\"patches/train/patch_{p}_{ndigit(3, f)}_{ndigit(5, j)}.npy\", np.array(patch, dtype=\"object\"))                                 \n",
    "                j += 1\n",
    "    print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "    print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "    print(\"Patched the pictures\")\n",
    "    print(\"NaN values:\", nan_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31235460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of ground truths\n",
    "# num = 0\n",
    "# path = [\"02\", \"train\"]\n",
    "# for p in path:\n",
    "#     for f in range(20):\n",
    "#         mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "#         ground_truths_pos = np.array(np.where(mask != 0)).T\n",
    "#         num = num + len(ground_truths_pos)\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fce53c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = 15\n",
    "#load_data(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patches\n",
    "directory = 'patches/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "dataset = [np.load(file_path, allow_pickle=True) for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541939c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset contains 38863 samples where each input shape is (15, 15, 12) and target shape is ().\n"
     ]
    }
   ],
   "source": [
    "# Check some metrics on the dataset\n",
    "l_t = len(dataset)\n",
    "X,y = dataset[0]\n",
    "X_s = X.shape\n",
    "y_s = y.shape\n",
    "print(f\"Trainset contains {l_t} samples where each input shape is {X_s} and target shape is {y_s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6705133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31090 7773\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of the training set and validation set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split trainset into trainset and valset\n",
    "trainset, valset = random_split(dataset, [train_size, val_size])\n",
    "print(len(trainset), len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80a20b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31090, 15, 15, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacks data for all train images in one array\n",
    "inputs = np.stack([data[0] for data in trainset], axis=0)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc31cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n",
      "[4.98844112e+02 7.37028340e+02 7.10326582e+02 1.18769262e+03\n",
      " 2.47062716e+03 2.93873134e+03 3.07531956e+03 3.20733533e+03\n",
      " 2.04721710e+03 1.24347876e+03 6.29820632e-01 2.25335066e-01] [2.80725031e+02 3.47797050e+02 5.02677539e+02 5.03825359e+02\n",
      " 6.36632688e+02 7.79218460e+02 8.44054597e+02 8.32966697e+02\n",
      " 6.79321531e+02 5.86655628e+02 2.13711499e-01 1.43264088e-01]\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and standard deviation for each channel from all pictures along heigth and width\n",
    "channel_means = np.mean(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_stds = np.std(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_meds = np.median(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_mins = np.min(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_maxs = np.max(inputs, axis=(0, 1, 2), keepdims=False)\n",
    "channel_10_quants = np.quantile(inputs, 0.1, axis=(0, 1, 2), keepdims=False)\n",
    "channel_90_quants = np.quantile(inputs, 0.9, axis=(0, 1, 2), keepdims=False)\n",
    "\n",
    "print(len(channel_means), len(channel_stds))\n",
    "print(channel_means, channel_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784d0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 1\n",
      "mean = 498.84411193309745\n",
      "std = 280.72503084489443\n",
      "median = 430.0\n",
      "min = 0.0\n",
      "max = 10912.0\n",
      "10 percent quantile = 236.0\n",
      "90 percent quantile = 865.0\n",
      "\n",
      "Channel 2\n",
      "mean = 737.028340230871\n",
      "std = 347.797049835219\n",
      "median = 693.0\n",
      "min = 0.0\n",
      "max = 11752.0\n",
      "10 percent quantile = 381.0\n",
      "90 percent quantile = 1211.0\n",
      "\n",
      "Channel 3\n",
      "mean = 710.3265818948572\n",
      "std = 502.6775393795845\n",
      "median = 548.0\n",
      "min = 0.0\n",
      "max = 13104.0\n",
      "10 percent quantile = 228.0\n",
      "90 percent quantile = 1462.0\n",
      "\n",
      "Channel 4\n",
      "mean = 1187.6926209928165\n",
      "std = 503.8253594015563\n",
      "median = 1155.0\n",
      "min = 0.0\n",
      "max = 10560.0\n",
      "10 percent quantile = 639.0\n",
      "90 percent quantile = 1894.0\n",
      "\n",
      "Channel 5\n",
      "mean = 2470.627163289375\n",
      "std = 636.6326883379342\n",
      "median = 2544.0\n",
      "min = 0.0\n",
      "max = 9914.0\n",
      "10 percent quantile = 1680.0\n",
      "90 percent quantile = 3175.0\n",
      "\n",
      "Channel 6\n",
      "mean = 2938.73134269683\n",
      "std = 779.2184604096918\n",
      "median = 2990.0\n",
      "min = 0.0\n",
      "max = 10352.0\n",
      "10 percent quantile = 1975.0\n",
      "90 percent quantile = 3856.0\n",
      "\n",
      "Channel 7\n",
      "mean = 3075.3195635609877\n",
      "std = 844.0545967596627\n",
      "median = 3138.0\n",
      "min = 0.0\n",
      "max = 12784.0\n",
      "10 percent quantile = 2040.0\n",
      "90 percent quantile = 4076.0\n",
      "\n",
      "Channel 8\n",
      "mean = 3207.335325256424\n",
      "std = 832.9666973568867\n",
      "median = 3280.0\n",
      "min = 0.0\n",
      "max = 9803.0\n",
      "10 percent quantile = 2170.0\n",
      "90 percent quantile = 4179.0\n",
      "\n",
      "Channel 9\n",
      "mean = 2047.217096458311\n",
      "std = 679.3215308036627\n",
      "median = 2067.0\n",
      "min = 0.0\n",
      "max = 10745.0\n",
      "10 percent quantile = 1189.0\n",
      "90 percent quantile = 2896.0\n",
      "\n",
      "Channel 10\n",
      "mean = 1243.478755155284\n",
      "std = 586.655628284463\n",
      "median = 1171.0\n",
      "min = 0.0\n",
      "max = 12144.0\n",
      "10 percent quantile = 585.0\n",
      "90 percent quantile = 2022.0\n",
      "\n",
      "Channel 11\n",
      "mean = 0.6298206323656903\n",
      "std = 0.21371149933455125\n",
      "median = 0.6908992997601241\n",
      "min = -0.7114228442656857\n",
      "max = 0.9374867104109107\n",
      "10 percent quantile = 0.3294663572130318\n",
      "90 percent quantile = 0.863176988633538\n",
      "\n",
      "Channel 12\n",
      "mean = 0.22533506622041885\n",
      "std = 0.1432640884075093\n",
      "median = 0.23802223069413142\n",
      "min = -0.5327607360655802\n",
      "max = 0.6200807263824216\n",
      "10 percent quantile = 0.024875621885832623\n",
      "90 percent quantile = 0.39956263661362657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(inputs.shape[3]):\n",
    "  print(f\"Channel {i+1}\")\n",
    "  print(f\"mean = {channel_means[i]}\")\n",
    "  print(f\"std = {channel_stds[i]}\")\n",
    "  print(f\"median = {channel_meds[i]}\")\n",
    "  print(f\"min = {channel_mins[i]}\")\n",
    "  print(f\"max = {channel_maxs[i]}\")\n",
    "  print(f\"10 percent quantile = {channel_10_quants[i]}\")\n",
    "  print(f\"90 percent quantile = {channel_90_quants[i]}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a187451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class in order to transform dataset and apply data augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, augmentations):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.augment = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "           data = self.augment(data)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c054c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom rotation transformation from the documentation in order to rotate at given angles,\n",
    "# not select from range of angles.\n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, image):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(image, angle)\n",
    "\n",
    "# Custom elastic transformation which adds randomness. Originally, transforms.ElasticTransform transforms\n",
    "# every image, but now only at given probability.\n",
    "class RandomElasticTransform:\n",
    "    def __init__(self, probability, alpha, sigma):\n",
    "        self.probability = probability\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if np.random.rand() < self.probability:\n",
    "          elastic_transformer = transforms.ElasticTransform(self.alpha, self.sigma)\n",
    "          return elastic_transformer(image)\n",
    "        else:\n",
    "          return image\n",
    "\n",
    "# Custom normalization class. Only the first 10 channels need to be normalized, the last two are already in normalized form\n",
    "# since their creation. The regular transforms.Normalize will throw an error, however, if less means/stds are passed than\n",
    "# there are channels.\n",
    "class MyNormalization:\n",
    "    def __init__(self, means, stds):\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        self.channels = [i for i in range(len(means))] # Channels to normalize\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i in self.channels:\n",
    "            image[i, :, :] = (image[i, :, :] - self.means[i]) / self.stds[i]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1841b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), # if input is 3D array then toTensor() switches dimensions from H x W x C to C x H x W\n",
    "     transforms.ConvertImageDtype(torch.float64),\n",
    "     #transforms.Lambda(lambda x : x / 3000),\n",
    "     #transforms.Lambda(lambda x : torch.where(x > 1, 1, x)), # fix pixel values between 0 and 1\n",
    "     MyNormalization(means=channel_means[0:10], # applies normalization with means and stds of trainset\n",
    "                          stds=channel_stds[0:10])\n",
    "     ])\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [MyRotationTransform(angles=[0, 90, 180, 270, 0]),\n",
    "     transforms.RandomAffine(degrees=0, translate=(0.2,0.2)), # shift in both directions along 0.5 * height on y-axis and 0.5 * width on x-axis\n",
    "                                                                                 # scale in range 0.25 <= scale <= 0.75\n",
    "     transforms.ElasticTransform(alpha=5.0, sigma=0.5), # displaces pixels\n",
    "     transforms.RandomHorizontalFlip(), # default p = 0.5\n",
    "     transforms.RandomVerticalFlip()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f0648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom trainset\n",
    "trainset_transformed = CustomDataset(trainset, transform=transform, augmentations=None)\n",
    "valset_transformed = CustomDataset(valset, transform=transform, augmentations=None)\n",
    "#trainset_transformed[0][0]\n",
    "#on the fly augmentation during training, hence no additional pictures in trainset\n",
    "#len(trainset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4efa000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8230, -0.8727, -0.6062,  ..., -0.2083,  0.2039,  0.6231],\n",
       "          [-0.7875, -0.9083, -0.7555,  ...,  0.2501,  0.4988,  0.6054],\n",
       "          [-0.7164, -0.8656, -0.7875,  ...,  0.3424,  0.4348,  0.5592],\n",
       "          ...,\n",
       "          [ 1.0673,  0.9749,  0.8115,  ...,  0.4633,  0.4704,  0.3638],\n",
       "          [ 1.0175,  0.9003,  0.8008,  ...,  0.3780,  0.4597,  0.3211],\n",
       "          [ 1.0175,  0.9287,  0.8186,  ...,  0.3034,  0.4277,  0.4029]],\n",
       " \n",
       "         [[-0.4963, -0.6256, -0.5250,  ...,  0.1791,  0.5957,  1.1934],\n",
       "          [-0.4934, -0.7003, -0.6974,  ...,  0.9032,  1.2682,  1.3199],\n",
       "          [-0.4158, -0.6572, -0.6486,  ...,  0.9865,  1.0584,  1.2480],\n",
       "          ...,\n",
       "          [ 1.6992,  1.5239,  1.5728,  ...,  1.3860,  1.3716,  1.0957],\n",
       "          [ 1.5613,  1.5182,  1.6187,  ...,  1.1791,  1.1791,  1.0067],\n",
       "          [ 1.3630,  1.6791,  1.6676,  ...,  1.0153,  1.1274,  1.0670]],\n",
       " \n",
       "         [[-0.7339, -0.8075, -0.5015,  ..., -0.0108,  0.5435,  0.8256],\n",
       "          [-0.6585, -0.8134, -0.6088,  ...,  0.9170,  1.1872,  1.4355],\n",
       "          [-0.5214, -0.7399, -0.6128,  ...,  1.0362,  1.1455,  1.3402],\n",
       "          ...,\n",
       "          [ 1.9004,  1.2786,  1.1991,  ...,  1.1534,  1.1196,  1.0501],\n",
       "          [ 1.6183,  1.0739,  1.3819,  ...,  1.0978,  1.1534,  0.9845],\n",
       "          [ 1.2686,  1.2110,  1.5309,  ...,  1.0461,  1.1455,  0.9826]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4934, -0.4559, -0.4559,  ...,  0.0379,  0.3988,  0.3988],\n",
       "          [-0.2295, -0.4389, -0.4389,  ...,  0.6661,  0.9947,  0.9947],\n",
       "          [-0.2295, -0.4389, -0.4389,  ...,  0.6661,  0.9947,  0.9947],\n",
       "          ...,\n",
       "          [ 1.1701,  1.3642,  1.3642,  ...,  0.1877,  0.1911,  0.1911],\n",
       "          [ 1.1513,  0.9300,  0.9300,  ...,  0.1826,  0.2030,  0.2030],\n",
       "          [ 1.1513,  0.9300,  0.9300,  ...,  0.1826,  0.2030,  0.2030]],\n",
       " \n",
       "         [[ 0.8336,  0.8406,  0.7077,  ...,  0.6601,  0.5647,  0.5203],\n",
       "          [ 0.7879,  0.8356,  0.7325,  ...,  0.4782,  0.4344,  0.3886],\n",
       "          [ 0.7165,  0.8040,  0.7292,  ...,  0.4483,  0.4290,  0.3922],\n",
       "          ...,\n",
       "          [ 0.3008,  0.4070,  0.4522,  ...,  0.4535,  0.4580,  0.4657],\n",
       "          [ 0.3456,  0.4521,  0.4451,  ...,  0.4633,  0.4567,  0.4785],\n",
       "          [ 0.4100,  0.4465,  0.4363,  ...,  0.4716,  0.4588,  0.4831]],\n",
       " \n",
       "         [[ 0.3122,  0.2796,  0.2796,  ...,  0.2100,  0.2112,  0.2112],\n",
       "          [ 0.2659,  0.2789,  0.2789,  ...,  0.1642,  0.1384,  0.1384],\n",
       "          [ 0.2659,  0.2789,  0.2789,  ...,  0.1642,  0.1384,  0.1384],\n",
       "          ...,\n",
       "          [ 0.0918,  0.1229,  0.1229,  ...,  0.2478,  0.2441,  0.2441],\n",
       "          [ 0.1114,  0.1915,  0.1915,  ...,  0.2544,  0.2463,  0.2463],\n",
       "          [ 0.1114,  0.1915,  0.1915,  ...,  0.2544,  0.2463,  0.2463]]],\n",
       "        dtype=torch.float64),\n",
       " 6.809999942779541)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for transformed training set and validation set\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(trainset_transformed, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validloader = DataLoader(valset_transformed, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit = 0\n",
    "#for batch in trainloader:\n",
    "\n",
    "    #while(limit <1):\n",
    "        #print(batch)\n",
    "    #    print(x.shape)\n",
    "    #    print(x.size)\n",
    "    #    print(y.shape)\n",
    "    #    print(y)\n",
    "        #limit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "    def __init__(self, *layers, classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = 0.01  # Assign the learning rate here\n",
    "        self.classes = classes\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)  # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_prefix='train'):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        y_hat = y_hat.flatten()\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(y_hat, y)\n",
    "        self.log(f\"{log_prefix}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam with Weight Decay\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        # ReduceLROnPlateau reduces the learning rate by 0.1 if the val_loss has not decreased within the last 10 epochs.\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "            # 'step' updates the scheduler after every step (alternative: 'epoch').\n",
    "            \"interval\": \"epoch\",\n",
    "            # Updates the learning rate after every step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to monitor for scheduler\n",
    "            \"monitor\": \"valid_loss\",\n",
    "            # Enforce that the value specified 'monitor' is available when the scheduler is updated, \n",
    "            # thus stopping training if not found.\n",
    "            \"strict\": True,\n",
    "            # No custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements entry to SepConv2d, see Lang et al. (2019), p. 6\n",
    "class MyEntryLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.proj_out = nn.Conv2d(in_channels, out_channels[len(out_channels)-1], (1,1))\n",
    "\n",
    "        self.entry_blocks = nn.ModuleList()\n",
    "        for i in range(len(out_channels)):\n",
    "            self.entry_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels[i], (1, 1)),\n",
    "                nn.BatchNorm2d(out_channels[i]),\n",
    "                nn.ReLU(inplace = True)\n",
    "            ))\n",
    "            in_channels = out_channels[i]  # Update in_channels for next iteration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_entry = x\n",
    "        for i in range(len(self.out_channels)):\n",
    "            x_entry = self.entry_blocks[i](x_entry)\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements SepConv2D\n",
    "class MySepConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "\n",
    "        self.sep_conv_block = nn.Sequential(\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, groups=in_channels, **kwargs), # depthwise SepConv\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs), # pointwise SepConv\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_sep_conv = self.sep_conv_block(x)\n",
    "        x_sep_conv_2 = self.sep_conv_block(x_sep_conv) # performs second SepConv, see Lang et al. (2019), p. 6\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_sep_conv_2) # adds original input and sep_conv_2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = MyCNNModel(\n",
    "    MyEntryLayer(12, [32, 64, 128]), # increase number of channels to 128\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    MySepConvLayer(128, 128, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(128, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='64', max_epochs=15,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=1),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(tree_model, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b50a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "batch = next(iter(trainloader))\n",
    "inputs = batch[0]\n",
    "inputs = inputs.float()\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = tree_model(inputs).flatten()\n",
    "\n",
    "true_heights = batch[1]\n",
    "print(\"Targets:\", batch[1])\n",
    "print(\"Target shape:\", batch[1].shape)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Prediction shape:\", predictions.shape)\n",
    "# expected batch size number of predictions for height !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "for batch in validloader:\n",
    "    max_ground = max(batch[1])\n",
    "    if(max_ground > max_val):\n",
    "        max_val = max_ground\n",
    "print(\"Max ground truth in validation set is: \", max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "\n",
    "num_classes = 10  # Number of size classes\n",
    "class_intervals = 4  # Interval between size classes\n",
    "# 10 classes of each 4 m because maximum height in trainset is 39.6 (see above)\n",
    "class_thresholds = [i * class_intervals for i in range(1, num_classes+1)]\n",
    "\n",
    "mse_total = [0.0] * num_classes\n",
    "mae_total = [0.0] * num_classes\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "true_heights_total = 0.0\n",
    "predictions_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(validloader, desc=\"Evaluation\")\n",
    "    for batch in progress_bar:\n",
    "        inputs, true_heights = batch[0].float(), batch[1].float()\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        predictions = tree_model(inputs)\n",
    "        true_heights = true_heights.view(-1, 1)\n",
    "\n",
    "        mse = F.mse_loss(predictions, true_heights, reduction='none').squeeze()\n",
    "        mae = F.l1_loss(predictions, true_heights, reduction='none').squeeze()\n",
    "\n",
    "        true_heights_total += true_heights.sum().item()\n",
    "        predictions_total += predictions.sum().item()\n",
    "\n",
    "        for i, threshold in enumerate(class_thresholds):\n",
    "            indices = (true_heights <= threshold).squeeze(1)\n",
    "            mse_total[i] += mse[indices].sum().item()\n",
    "            mae_total[i] += mae[indices].sum().item()\n",
    "            class_counts[i] += indices.sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({'Total MSE': mse_total[0] / class_counts[0], 'Total MAE': mae_total[0] / class_counts[0]})\n",
    "\n",
    "mse_class_avg = [mse_total[i] / class_counts[i] if class_counts[i] != 0 else 0.0 for i in range(num_classes)]\n",
    "mae_class_avg = [mae_total[i] / class_counts[i] if class_counts[i] != 0 else 0.0 for i in range(num_classes)]\n",
    "average_true_height = true_heights_total / len(validloader.dataset)\n",
    "average_prediction = predictions_total / len(validloader.dataset)\n",
    "# Calculate overall MSE and MAE\n",
    "overall_mse = sum(mse_total) / sum(class_counts) if sum(class_counts) != 0 else 0.0\n",
    "overall_mae = sum(mae_total) / sum(class_counts) if sum(class_counts) != 0 else 0.0\n",
    "\n",
    "\n",
    "# Print the evaluation metrics for each size class\n",
    "for i, threshold in enumerate(class_thresholds):\n",
    "    print(f\"Size Class {i+1}:\")\n",
    "    print(f\"MSE: {mse_class_avg[i]}\")\n",
    "    print(f\"MAE: {mae_class_avg[i]}\")\n",
    "\n",
    "# Print the overall evaluation metrics\n",
    "print(\"Average True Height:\", average_true_height)\n",
    "print(\"Average Prediction:\", average_prediction)\n",
    "print(\"Average MSE: \", overall_mse )\n",
    "print(\"Average MAE:\", overall_mae )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aff022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6eb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5660a571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2.52480411529541\n",
      "9\n",
      "2.8013558387756348\n",
      "10\n",
      "2.4413347244262695\n",
      "11\n",
      "2.8970251083374023\n",
      "12\n",
      "3.388537883758545\n",
      "13\n",
      "3.58461856842041\n",
      "14\n",
      "3.3386659622192383\n",
      "15\n",
      "3.4727354049682617\n",
      "16\n",
      "3.1547937393188477\n",
      "17\n",
      "3.282163619995117\n",
      "18\n",
      "3.7190370559692383\n",
      "19\n",
      "4.402204513549805\n",
      "20\n",
      "5.386014461517334\n",
      "21\n",
      "5.979381561279297\n",
      "22\n",
      "5.4265642166137695\n",
      "23\n",
      "3.631992816925049\n",
      "24\n",
      "3.831084728240967\n",
      "25\n",
      "3.6621880531311035\n",
      "26\n",
      "4.619596481323242\n",
      "27\n",
      "4.6498122215271\n",
      "28\n",
      "6.237827301025391\n",
      "29\n",
      "6.534231185913086\n",
      "30\n",
      "4.253218650817871\n",
      "31\n",
      "4.2867560386657715\n",
      "32\n",
      "3.9681453704833984\n",
      "33\n",
      "4.569218158721924\n",
      "34\n",
      "4.281702041625977\n",
      "35\n",
      "4.158605098724365\n",
      "36\n",
      "4.435638427734375\n",
      "37\n",
      "4.126987457275391\n",
      "38\n",
      "4.3706817626953125\n",
      "39\n",
      "4.322046279907227\n",
      "40\n",
      "4.635588645935059\n",
      "41\n",
      "4.83427095413208\n",
      "42\n",
      "5.4300856590271\n",
      "43\n",
      "5.46923303604126\n",
      "44\n",
      "6.1143798828125\n",
      "45\n",
      "6.411104202270508\n",
      "46\n",
      "5.82167911529541\n",
      "47\n",
      "6.102870941162109\n",
      "48\n",
      "5.333505153656006\n",
      "49\n",
      "5.7792181968688965\n",
      "50\n",
      "5.036778450012207\n",
      "51\n",
      "4.85888671875\n",
      "52\n",
      "4.8764448165893555\n",
      "53\n",
      "4.684939384460449\n",
      "54\n",
      "4.597720623016357\n",
      "55\n",
      "4.707838535308838\n",
      "56\n",
      "5.112056255340576\n",
      "57\n",
      "5.346675872802734\n",
      "58\n",
      "5.226714134216309\n",
      "59\n",
      "6.220332622528076\n",
      "60\n",
      "5.980188369750977\n",
      "61\n",
      "6.387465953826904\n",
      "62\n",
      "6.00577974319458\n",
      "63\n",
      "5.203622817993164\n",
      "64\n",
      "5.0832295417785645\n",
      "65\n",
      "4.749719619750977\n",
      "66\n",
      "4.773266792297363\n",
      "67\n",
      "4.139500617980957\n",
      "68\n",
      "4.083192825317383\n",
      "69\n",
      "3.8222222328186035\n",
      "70\n",
      "3.67887020111084\n",
      "71\n",
      "3.5350961685180664\n",
      "72\n",
      "3.460110664367676\n",
      "73\n",
      "3.1665401458740234\n",
      "74\n",
      "2.979771614074707\n",
      "75\n",
      "2.568570613861084\n",
      "76\n",
      "2.311837911605835\n",
      "77\n",
      "2.205859661102295\n",
      "78\n",
      "2.0507755279541016\n",
      "79\n",
      "1.7674341201782227\n",
      "80\n",
      "1.7171010971069336\n",
      "81\n",
      "1.649192214012146\n",
      "82\n",
      "1.6361916065216064\n",
      "83\n",
      "1.5951592922210693\n",
      "84\n",
      "1.5978796482086182\n",
      "85\n",
      "1.6115672588348389\n",
      "86\n",
      "1.5991653203964233\n",
      "87\n",
      "1.6036803722381592\n",
      "88\n",
      "1.6146037578582764\n",
      "89\n",
      "1.6485872268676758\n",
      "90\n",
      "1.6511456966400146\n",
      "91\n",
      "1.6459007263183594\n",
      "92\n",
      "1.6409685611724854\n",
      "93\n",
      "1.644458532333374\n",
      "94\n",
      "1.6344764232635498\n",
      "95\n",
      "1.637366771697998\n",
      "96\n",
      "1.6359840631484985\n",
      "97\n",
      "1.6316416263580322\n",
      "98\n",
      "1.669856071472168\n",
      "99\n",
      "1.674093246459961\n",
      "100\n",
      "1.648212194442749\n",
      "101\n",
      "1.6243799924850464\n",
      "102\n",
      "1.5900733470916748\n",
      "103\n",
      "1.6622081995010376\n",
      "104\n",
      "1.6824166774749756\n",
      "105\n",
      "1.7609002590179443\n",
      "106\n",
      "1.7182502746582031\n",
      "107\n",
      "1.7791016101837158\n",
      "108\n",
      "1.80201256275177\n",
      "109\n",
      "2.0959765911102295\n",
      "110\n",
      "2.2117042541503906\n",
      "111\n",
      "2.2137703895568848\n",
      "112\n",
      "2.257242202758789\n",
      "113\n",
      "2.190080165863037\n",
      "114\n",
      "2.1688356399536133\n",
      "115\n",
      "2.1073250770568848\n",
      "116\n",
      "1.8828322887420654\n",
      "117\n",
      "1.9479115009307861\n",
      "118\n",
      "2.0429697036743164\n",
      "119\n",
      "2.043792724609375\n",
      "120\n",
      "2.1315622329711914\n",
      "121\n",
      "2.266629934310913\n",
      "122\n",
      "2.284055709838867\n",
      "123\n",
      "2.347503900527954\n",
      "124\n",
      "2.328613758087158\n",
      "125\n",
      "2.5232975482940674\n",
      "126\n",
      "2.4855566024780273\n",
      "127\n",
      "2.3867886066436768\n",
      "128\n",
      "2.3660428524017334\n",
      "129\n",
      "2.516742467880249\n",
      "130\n",
      "2.558770179748535\n",
      "131\n",
      "2.992433547973633\n",
      "132\n",
      "2.9864578247070312\n",
      "133\n",
      "3.1956934928894043\n",
      "134\n",
      "3.1231675148010254\n",
      "135\n",
      "2.6710691452026367\n",
      "136\n",
      "2.684584856033325\n",
      "137\n",
      "2.554718494415283\n",
      "138\n",
      "2.531683921813965\n",
      "139\n",
      "2.5949857234954834\n",
      "140\n",
      "2.7139012813568115\n",
      "141\n",
      "2.648386001586914\n",
      "142\n",
      "2.608811378479004\n",
      "143\n",
      "2.481157064437866\n",
      "144\n",
      "2.207324504852295\n",
      "145\n",
      "2.035348415374756\n",
      "146\n",
      "2.129636287689209\n",
      "147\n",
      "2.2226099967956543\n",
      "148\n",
      "2.371159791946411\n",
      "149\n",
      "2.427628993988037\n",
      "150\n",
      "2.587777614593506\n",
      "151\n",
      "2.595064640045166\n",
      "152\n",
      "2.452047824859619\n",
      "153\n",
      "2.4315199851989746\n",
      "154\n",
      "2.7693328857421875\n",
      "155\n",
      "2.7781519889831543\n",
      "156\n",
      "2.7289137840270996\n",
      "157\n",
      "2.8120226860046387\n",
      "158\n",
      "2.8239030838012695\n",
      "159\n",
      "2.8545737266540527\n",
      "160\n",
      "3.1921815872192383\n",
      "161\n",
      "3.357090950012207\n",
      "162\n",
      "3.3442630767822266\n",
      "163\n",
      "3.3119301795959473\n",
      "164\n",
      "3.3009634017944336\n",
      "165\n",
      "3.2435426712036133\n",
      "166\n",
      "3.3086295127868652\n",
      "167\n",
      "3.296043872833252\n",
      "168\n",
      "3.241694927215576\n",
      "169\n",
      "3.182948589324951\n",
      "170\n",
      "2.8698906898498535\n",
      "171\n",
      "2.8570985794067383\n",
      "172\n",
      "2.751272201538086\n",
      "173\n",
      "2.715204954147339\n",
      "174\n",
      "2.784512996673584\n",
      "175\n",
      "2.754063129425049\n",
      "176\n",
      "2.901172161102295\n",
      "177\n",
      "2.938812732696533\n",
      "178\n",
      "3.0069046020507812\n",
      "179\n",
      "3.1151962280273438\n",
      "180\n",
      "3.08365797996521\n",
      "181\n",
      "3.151566505432129\n",
      "182\n",
      "3.3907604217529297\n",
      "183\n",
      "3.5127696990966797\n",
      "184\n",
      "3.604750633239746\n",
      "185\n",
      "3.6900553703308105\n",
      "186\n",
      "3.770329475402832\n",
      "187\n",
      "3.7356019020080566\n",
      "188\n",
      "3.7305121421813965\n",
      "189\n",
      "3.784879684448242\n",
      "190\n",
      "3.752284526824951\n",
      "191\n",
      "3.853058338165283\n",
      "192\n",
      "3.6937685012817383\n",
      "193\n",
      "3.805919647216797\n",
      "194\n",
      "3.826974391937256\n",
      "195\n",
      "4.22589111328125\n",
      "196\n",
      "4.35652494430542\n",
      "197\n",
      "4.532250881195068\n",
      "198\n",
      "4.600422382354736\n",
      "199\n",
      "4.550642967224121\n",
      "200\n",
      "4.751334190368652\n",
      "201\n",
      "5.43950080871582\n",
      "202\n",
      "6.189295291900635\n",
      "203\n",
      "6.638700485229492\n",
      "204\n",
      "7.248682498931885\n",
      "205\n",
      "7.212697982788086\n",
      "206\n",
      "5.970530986785889\n",
      "207\n",
      "5.702356338500977\n",
      "208\n",
      "5.04092264175415\n",
      "209\n",
      "4.677901744842529\n",
      "210\n",
      "4.731426239013672\n",
      "211\n",
      "5.185770511627197\n",
      "212\n",
      "5.7972025871276855\n",
      "213\n",
      "5.6875386238098145\n",
      "214\n",
      "5.545907497406006\n",
      "215\n",
      "5.316611289978027\n",
      "216\n",
      "5.787308216094971\n",
      "217\n",
      "5.452834606170654\n",
      "218\n",
      "6.012128829956055\n",
      "219\n",
      "6.149910926818848\n",
      "220\n",
      "7.195120811462402\n",
      "221\n",
      "7.6814446449279785\n",
      "222\n",
      "8.344569206237793\n",
      "223\n",
      "8.372359275817871\n",
      "224\n",
      "9.107672691345215\n",
      "225\n",
      "6.122842311859131\n",
      "226\n",
      "6.003749370574951\n",
      "227\n",
      "4.7439727783203125\n",
      "228\n",
      "4.365588188171387\n",
      "229\n",
      "3.7101612091064453\n",
      "230\n",
      "3.4378857612609863\n",
      "231\n",
      "3.273691177368164\n",
      "232\n",
      "3.039077043533325\n",
      "233\n",
      "3.240478038787842\n",
      "234\n",
      "3.2098803520202637\n",
      "235\n",
      "4.084198951721191\n",
      "236\n",
      "4.123974323272705\n",
      "237\n",
      "4.022045135498047\n",
      "238\n",
      "4.180714130401611\n",
      "239\n",
      "4.565009117126465\n",
      "240\n",
      "4.681483745574951\n",
      "241\n",
      "5.736892223358154\n",
      "242\n",
      "5.108987331390381\n",
      "243\n",
      "5.709583282470703\n",
      "244\n",
      "4.973289489746094\n",
      "245\n",
      "4.765585899353027\n",
      "246\n",
      "5.10468053817749\n",
      "247\n",
      "5.832676410675049\n",
      "248\n",
      "6.019020080566406\n",
      "249\n",
      "5.436432361602783\n",
      "250\n",
      "4.809269428253174\n",
      "251\n",
      "3.969897747039795\n",
      "252\n",
      "4.016758918762207\n",
      "253\n",
      "4.218275547027588\n",
      "254\n",
      "4.317646026611328\n",
      "255\n",
      "4.347315788269043\n",
      "256\n",
      "4.223947048187256\n",
      "257\n",
      "3.428349018096924\n",
      "258\n",
      "3.4673447608947754\n",
      "259\n",
      "3.1363954544067383\n",
      "260\n",
      "3.118314743041992\n",
      "261\n",
      "2.5728366374969482\n",
      "262\n",
      "2.3893537521362305\n",
      "263\n",
      "2.478699207305908\n",
      "264\n",
      "2.331592559814453\n",
      "265\n",
      "2.247849941253662\n",
      "266\n",
      "2.0330116748809814\n",
      "267\n",
      "2.0319175720214844\n",
      "268\n",
      "2.0370779037475586\n",
      "269\n",
      "2.0184202194213867\n",
      "270\n",
      "1.7773574590682983\n",
      "271\n",
      "1.7946279048919678\n",
      "272\n",
      "1.7686102390289307\n",
      "273\n",
      "1.7776031494140625\n",
      "274\n",
      "1.7785706520080566\n",
      "275\n",
      "1.7915606498718262\n",
      "276\n",
      "1.781008005142212\n",
      "277\n",
      "1.7686983346939087\n",
      "278\n",
      "1.779677391052246\n",
      "279\n",
      "1.7727601528167725\n",
      "280\n",
      "1.7438123226165771\n",
      "281\n",
      "1.7175077199935913\n",
      "282\n",
      "1.6825065612792969\n",
      "283\n",
      "1.6543314456939697\n",
      "284\n",
      "1.6028802394866943\n",
      "285\n",
      "1.6901527643203735\n",
      "286\n",
      "1.6622226238250732\n",
      "287\n",
      "1.7243969440460205\n",
      "288\n",
      "1.7322769165039062\n",
      "289\n",
      "1.7082128524780273\n",
      "290\n",
      "1.6674580574035645\n",
      "291\n",
      "1.918726921081543\n",
      "292\n",
      "2.131013870239258\n",
      "293\n",
      "2.46101713180542\n",
      "294\n",
      "2.650350570678711\n",
      "295\n",
      "2.872622489929199\n",
      "296\n",
      "2.725557804107666\n",
      "297\n",
      "2.81482195854187\n",
      "298\n",
      "2.770888566970825\n",
      "299\n",
      "3.2482852935791016\n",
      "300\n",
      "3.3858656883239746\n",
      "301\n",
      "4.077030658721924\n",
      "302\n",
      "3.8720197677612305\n",
      "303\n",
      "4.582371711730957\n",
      "304\n",
      "3.925813674926758\n",
      "305\n",
      "3.5892796516418457\n",
      "306\n",
      "3.3994412422180176\n",
      "307\n",
      "3.5855636596679688\n",
      "308\n",
      "3.618497371673584\n",
      "309\n",
      "3.711764335632324\n",
      "310\n",
      "3.62839937210083\n",
      "311\n",
      "3.6318416595458984\n",
      "312\n",
      "4.3070502281188965\n",
      "313\n",
      "4.081329345703125\n",
      "314\n",
      "5.883196830749512\n",
      "315\n",
      "5.7297282218933105\n",
      "316\n",
      "6.010159969329834\n",
      "317\n",
      "5.1569743156433105\n",
      "318\n",
      "4.959367752075195\n",
      "319\n",
      "4.8316521644592285\n",
      "320\n",
      "3.850757598876953\n",
      "321\n",
      "4.597681045532227\n",
      "322\n",
      "4.601170539855957\n",
      "323\n",
      "4.497346878051758\n",
      "324\n",
      "4.874582767486572\n",
      "325\n",
      "5.530290603637695\n",
      "326\n",
      "6.158813953399658\n",
      "327\n",
      "5.0400071144104\n",
      "328\n",
      "5.6810808181762695\n",
      "329\n",
      "7.086123943328857\n",
      "330\n",
      "7.803341865539551\n",
      "331\n",
      "9.411615371704102\n",
      "332\n",
      "10.199039459228516\n",
      "333\n",
      "9.778712272644043\n",
      "334\n",
      "8.587814331054688\n",
      "335\n",
      "7.9637298583984375\n",
      "336\n",
      "8.243035316467285\n",
      "337\n",
      "7.702002048492432\n",
      "338\n",
      "6.721118927001953\n",
      "339\n",
      "5.123796463012695\n",
      "340\n",
      "5.234242916107178\n",
      "341\n",
      "4.5574822425842285\n",
      "342\n",
      "4.3075056076049805\n",
      "343\n",
      "3.5937371253967285\n",
      "344\n",
      "3.494049549102783\n",
      "345\n",
      "3.6201958656311035\n",
      "346\n",
      "3.598236083984375\n",
      "347\n",
      "3.563915252685547\n",
      "348\n",
      "3.5521535873413086\n",
      "349\n",
      "3.656190872192383\n",
      "350\n",
      "3.711806297302246\n",
      "351\n",
      "3.937295436859131\n",
      "352\n",
      "4.030463218688965\n",
      "353\n",
      "3.5619726181030273\n",
      "354\n",
      "3.632148265838623\n",
      "355\n",
      "3.8521761894226074\n",
      "356\n",
      "4.92415714263916\n",
      "357\n",
      "5.954249382019043\n",
      "358\n",
      "7.488053798675537\n",
      "359\n",
      "8.189061164855957\n",
      "360\n",
      "8.344721794128418\n",
      "361\n",
      "7.0880889892578125\n",
      "362\n",
      "6.632885456085205\n",
      "363\n",
      "5.811675071716309\n",
      "364\n",
      "6.232436656951904\n",
      "365\n",
      "4.9607367515563965\n",
      "366\n",
      "4.817353248596191\n",
      "367\n",
      "4.2134833335876465\n",
      "368\n",
      "4.197720527648926\n",
      "369\n",
      "3.8321595191955566\n",
      "370\n",
      "3.690310478210449\n",
      "371\n",
      "3.411719799041748\n",
      "372\n",
      "3.498474597930908\n",
      "373\n",
      "3.308285713195801\n",
      "374\n",
      "3.231393337249756\n",
      "375\n",
      "3.1587605476379395\n",
      "376\n",
      "2.6187281608581543\n",
      "377\n",
      "1.7256557941436768\n",
      "378\n",
      "1.7078371047973633\n",
      "379\n",
      "1.8106653690338135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "1.8383605480194092\n",
      "381\n",
      "2.1284589767456055\n",
      "382\n",
      "2.1267623901367188\n",
      "383\n",
      "2.261002779006958\n",
      "384\n",
      "2.2080631256103516\n",
      "385\n",
      "2.2157177925109863\n",
      "386\n",
      "2.204495429992676\n",
      "387\n",
      "2.302908182144165\n",
      "388\n",
      "2.3319478034973145\n",
      "389\n",
      "2.326199769973755\n",
      "390\n",
      "2.1894264221191406\n",
      "391\n",
      "2.084944248199463\n",
      "392\n",
      "2.043445587158203\n",
      "393\n",
      "2.121497631072998\n",
      "394\n",
      "2.112518310546875\n",
      "395\n",
      "2.0996413230895996\n",
      "396\n",
      "2.0743188858032227\n",
      "397\n",
      "2.0265023708343506\n",
      "398\n",
      "1.9489312171936035\n",
      "399\n",
      "1.9570538997650146\n",
      "400\n",
      "1.92239511013031\n",
      "401\n",
      "1.8954353332519531\n",
      "402\n",
      "1.9765640497207642\n",
      "403\n",
      "1.924695372581482\n",
      "404\n",
      "1.8114765882492065\n",
      "405\n",
      "1.8049414157867432\n",
      "406\n",
      "1.810539722442627\n",
      "407\n",
      "1.837540864944458\n",
      "408\n",
      "1.8115568161010742\n",
      "409\n",
      "1.773118495941162\n",
      "410\n",
      "1.6925389766693115\n",
      "411\n",
      "1.675654649734497\n",
      "412\n",
      "1.6274075508117676\n",
      "413\n",
      "1.6090893745422363\n",
      "414\n",
      "1.6123149394989014\n",
      "415\n",
      "1.6632013320922852\n",
      "416\n",
      "1.6661882400512695\n",
      "417\n",
      "1.7427126169204712\n",
      "418\n",
      "1.7636940479278564\n",
      "419\n",
      "1.8293366432189941\n",
      "420\n",
      "1.8388803005218506\n",
      "421\n",
      "1.9055366516113281\n",
      "422\n",
      "1.9511902332305908\n",
      "423\n",
      "2.024031639099121\n",
      "424\n",
      "2.0500741004943848\n",
      "425\n",
      "2.096406936645508\n",
      "426\n",
      "2.1444201469421387\n",
      "427\n",
      "2.0774643421173096\n",
      "428\n",
      "2.089780807495117\n",
      "429\n",
      "2.0691802501678467\n",
      "430\n",
      "2.117905616760254\n",
      "431\n",
      "2.1128852367401123\n",
      "432\n",
      "2.1575894355773926\n",
      "433\n",
      "2.2334303855895996\n",
      "434\n",
      "2.183316707611084\n",
      "435\n",
      "2.2837514877319336\n",
      "436\n",
      "2.207467794418335\n",
      "437\n",
      "2.1392345428466797\n",
      "438\n",
      "2.0644583702087402\n",
      "439\n",
      "2.0800929069519043\n",
      "440\n",
      "2.0194501876831055\n",
      "441\n",
      "2.0074915885925293\n",
      "442\n",
      "1.9056479930877686\n",
      "443\n",
      "1.971681833267212\n",
      "444\n",
      "2.2291884422302246\n",
      "445\n",
      "2.319753646850586\n",
      "446\n",
      "2.3075151443481445\n",
      "447\n",
      "2.3086066246032715\n",
      "448\n",
      "2.277374267578125\n",
      "449\n",
      "2.4201459884643555\n",
      "450\n",
      "2.774484157562256\n",
      "451\n",
      "2.746277332305908\n",
      "452\n",
      "2.8307530879974365\n",
      "453\n",
      "2.8257853984832764\n",
      "454\n",
      "2.650313138961792\n",
      "455\n",
      "2.6480305194854736\n",
      "456\n",
      "2.5858850479125977\n",
      "457\n",
      "2.5114798545837402\n",
      "458\n",
      "2.356318950653076\n",
      "459\n",
      "2.1551742553710938\n",
      "460\n",
      "2.108335018157959\n",
      "461\n",
      "2.107435464859009\n",
      "462\n",
      "2.083993434906006\n",
      "463\n",
      "2.017700433731079\n",
      "464\n",
      "1.9585225582122803\n",
      "465\n",
      "1.9984233379364014\n",
      "466\n",
      "1.9645593166351318\n",
      "467\n",
      "1.946087121963501\n",
      "468\n",
      "1.9122490882873535\n",
      "469\n",
      "1.9247275590896606\n",
      "470\n",
      "1.903922200202942\n",
      "471\n",
      "1.8660780191421509\n",
      "472\n",
      "1.7983677387237549\n",
      "473\n",
      "1.8448318243026733\n",
      "474\n",
      "1.8500242233276367\n",
      "475\n",
      "1.9589197635650635\n",
      "476\n",
      "1.9478332996368408\n",
      "477\n",
      "2.0454344749450684\n",
      "478\n",
      "1.9378983974456787\n",
      "479\n",
      "1.7514032125473022\n",
      "480\n",
      "1.6448475122451782\n",
      "481\n",
      "1.6226398944854736\n",
      "482\n",
      "1.5967727899551392\n",
      "483\n",
      "1.575750708580017\n",
      "484\n",
      "1.5069291591644287\n",
      "485\n",
      "1.5519156455993652\n",
      "486\n",
      "1.5977407693862915\n",
      "487\n",
      "1.5805778503417969\n",
      "488\n",
      "1.6131011247634888\n",
      "489\n",
      "1.6375391483306885\n",
      "490\n",
      "1.6555652618408203\n",
      "491\n",
      "1.6846635341644287\n",
      "492\n",
      "1.7342073917388916\n",
      "493\n",
      "1.7275893688201904\n",
      "494\n",
      "1.6833205223083496\n",
      "495\n",
      "1.662375807762146\n",
      "496\n",
      "1.6167693138122559\n",
      "497\n",
      "1.610141396522522\n",
      "498\n",
      "1.5832183361053467\n",
      "499\n",
      "1.5400687456130981\n",
      "500\n",
      "1.522761583328247\n",
      "501\n",
      "1.5378305912017822\n",
      "502\n",
      "1.5348424911499023\n",
      "503\n",
      "1.6461176872253418\n",
      "504\n",
      "1.6587451696395874\n",
      "505\n",
      "1.7219598293304443\n",
      "506\n",
      "1.74129056930542\n",
      "507\n",
      "1.793161153793335\n",
      "508\n",
      "1.8367769718170166\n",
      "509\n",
      "1.8228614330291748\n",
      "510\n",
      "1.7770053148269653\n",
      "511\n",
      "1.7314932346343994\n",
      "512\n",
      "1.7056224346160889\n",
      "513\n",
      "1.685110330581665\n",
      "514\n",
      "1.6740531921386719\n",
      "515\n",
      "1.6818673610687256\n",
      "516\n",
      "1.692600965499878\n",
      "517\n",
      "1.7389531135559082\n",
      "518\n",
      "1.8308496475219727\n",
      "519\n",
      "1.8335338830947876\n",
      "520\n",
      "1.7395907640457153\n",
      "521\n",
      "1.7568790912628174\n",
      "522\n",
      "1.672766923904419\n",
      "523\n",
      "1.5941963195800781\n",
      "524\n",
      "1.5453745126724243\n",
      "525\n",
      "1.5522375106811523\n",
      "526\n",
      "1.5087196826934814\n",
      "527\n",
      "1.5522825717926025\n",
      "528\n",
      "1.5256807804107666\n",
      "529\n",
      "1.5370817184448242\n",
      "530\n",
      "1.5569324493408203\n",
      "531\n",
      "1.6229058504104614\n",
      "532\n",
      "1.7381069660186768\n",
      "533\n",
      "2.7593016624450684\n",
      "534\n",
      "3.7423596382141113\n",
      "535\n",
      "4.150074005126953\n",
      "536\n",
      "3.9101967811584473\n",
      "537\n",
      "4.036831378936768\n",
      "538\n",
      "3.5999021530151367\n",
      "539\n",
      "4.131004810333252\n",
      "540\n",
      "4.461288928985596\n",
      "541\n",
      "5.080373287200928\n",
      "542\n",
      "4.659979820251465\n",
      "543\n",
      "4.834397792816162\n",
      "544\n",
      "4.464028835296631\n",
      "545\n",
      "4.234347820281982\n",
      "546\n",
      "4.092896461486816\n",
      "547\n",
      "3.9613828659057617\n",
      "548\n",
      "3.643123149871826\n",
      "549\n",
      "2.5384936332702637\n",
      "550\n",
      "1.8888523578643799\n",
      "551\n",
      "1.8668224811553955\n",
      "552\n",
      "1.813907504081726\n",
      "553\n",
      "1.9192535877227783\n",
      "554\n",
      "1.949141263961792\n",
      "555\n",
      "1.9768146276474\n",
      "556\n",
      "1.9471299648284912\n",
      "557\n",
      "2.024064540863037\n",
      "558\n",
      "1.9978352785110474\n",
      "559\n",
      "2.0012757778167725\n",
      "560\n",
      "2.0301294326782227\n",
      "561\n",
      "2.077922821044922\n",
      "562\n",
      "2.0850939750671387\n",
      "563\n",
      "2.125673532485962\n",
      "564\n",
      "2.171719551086426\n",
      "565\n",
      "2.3420321941375732\n",
      "566\n",
      "2.535801649093628\n",
      "567\n",
      "2.595477342605591\n",
      "568\n",
      "2.6597023010253906\n",
      "569\n",
      "2.7085986137390137\n",
      "570\n",
      "2.6676387786865234\n",
      "571\n",
      "2.6963844299316406\n",
      "572\n",
      "2.7516517639160156\n",
      "573\n",
      "3.016908884048462\n",
      "574\n",
      "3.0370240211486816\n",
      "575\n",
      "3.317697048187256\n",
      "576\n",
      "3.1433610916137695\n",
      "577\n",
      "2.7304720878601074\n",
      "578\n",
      "2.4800150394439697\n",
      "579\n",
      "2.334456205368042\n",
      "580\n",
      "2.157634973526001\n",
      "581\n",
      "1.9478728771209717\n",
      "582\n",
      "1.9134039878845215\n",
      "583\n",
      "1.802209734916687\n",
      "584\n",
      "1.7039403915405273\n",
      "585\n",
      "1.641622543334961\n",
      "586\n",
      "1.9013985395431519\n",
      "587\n",
      "1.9094663858413696\n",
      "588\n",
      "1.848840594291687\n",
      "589\n",
      "2.084773063659668\n",
      "590\n",
      "2.298552989959717\n",
      "591\n",
      "2.3653459548950195\n",
      "592\n",
      "2.4161295890808105\n",
      "593\n",
      "2.737626791000366\n",
      "594\n",
      "2.8306961059570312\n",
      "595\n",
      "3.0397839546203613\n",
      "596\n",
      "3.1496129035949707\n",
      "597\n",
      "3.2610554695129395\n",
      "598\n",
      "3.3020520210266113\n",
      "599\n",
      "3.4406495094299316\n",
      "600\n",
      "3.0679666996002197\n",
      "601\n",
      "3.294229507446289\n",
      "602\n",
      "3.2154202461242676\n",
      "603\n",
      "3.945164203643799\n",
      "604\n",
      "3.8271284103393555\n",
      "605\n",
      "4.469002723693848\n",
      "606\n",
      "4.151880741119385\n",
      "607\n",
      "3.3481807708740234\n",
      "608\n",
      "3.15645170211792\n",
      "609\n",
      "2.925558567047119\n",
      "610\n",
      "2.7913641929626465\n",
      "611\n",
      "2.4682459831237793\n",
      "612\n",
      "2.341172695159912\n",
      "613\n",
      "2.337278366088867\n",
      "614\n",
      "2.289579391479492\n",
      "615\n",
      "2.475881338119507\n",
      "616\n",
      "2.716862201690674\n",
      "617\n",
      "3.0800576210021973\n",
      "618\n",
      "3.1236367225646973\n",
      "619\n",
      "3.76779842376709\n",
      "620\n",
      "3.5378642082214355\n",
      "621\n",
      "3.1963610649108887\n",
      "622\n",
      "2.7833282947540283\n",
      "623\n",
      "2.732463836669922\n",
      "624\n",
      "2.553152561187744\n",
      "625\n",
      "2.360319137573242\n",
      "626\n",
      "2.2437691688537598\n",
      "627\n",
      "2.2359886169433594\n",
      "628\n",
      "2.260328769683838\n",
      "629\n",
      "2.380540370941162\n",
      "630\n",
      "2.362607002258301\n",
      "631\n",
      "2.3833706378936768\n",
      "632\n",
      "2.2357912063598633\n",
      "633\n",
      "2.2395031452178955\n",
      "634\n",
      "2.2238478660583496\n",
      "635\n",
      "2.332211971282959\n",
      "636\n",
      "2.3144288063049316\n",
      "637\n",
      "2.328369617462158\n",
      "638\n",
      "2.1864943504333496\n",
      "639\n",
      "2.1679039001464844\n",
      "640\n",
      "2.2033281326293945\n",
      "641\n",
      "2.24509334564209\n",
      "642\n",
      "2.2145562171936035\n",
      "643\n",
      "2.327838897705078\n",
      "644\n",
      "2.340144634246826\n",
      "645\n",
      "2.370328426361084\n",
      "646\n",
      "2.3758349418640137\n",
      "647\n",
      "2.378328800201416\n",
      "648\n",
      "2.3189353942871094\n",
      "649\n",
      "2.154270648956299\n",
      "650\n",
      "2.056002616882324\n",
      "651\n",
      "1.9662805795669556\n",
      "652\n",
      "1.912214756011963\n",
      "653\n",
      "1.856801986694336\n",
      "654\n",
      "1.8156166076660156\n",
      "655\n",
      "1.8836724758148193\n",
      "656\n",
      "1.9005241394042969\n",
      "657\n",
      "1.7393254041671753\n",
      "658\n",
      "1.718106985092163\n",
      "659\n",
      "1.6954090595245361\n",
      "660\n",
      "1.6581724882125854\n",
      "661\n",
      "1.7211382389068604\n",
      "662\n",
      "1.7404289245605469\n",
      "663\n",
      "1.7970232963562012\n",
      "664\n",
      "1.7808833122253418\n",
      "665\n",
      "1.7776085138320923\n",
      "666\n",
      "1.7322328090667725\n",
      "667\n",
      "1.719184160232544\n",
      "668\n",
      "1.7222285270690918\n",
      "669\n",
      "1.759070634841919\n",
      "670\n",
      "1.7937320470809937\n",
      "671\n",
      "1.7949250936508179\n",
      "672\n",
      "1.7793700695037842\n",
      "673\n",
      "1.8034541606903076\n",
      "674\n",
      "1.7953152656555176\n",
      "675\n",
      "1.7466531991958618\n",
      "676\n",
      "1.7016255855560303\n",
      "677\n",
      "1.674837589263916\n",
      "678\n",
      "1.6318564414978027\n",
      "679\n",
      "1.661461353302002\n",
      "680\n",
      "1.6524872779846191\n",
      "681\n",
      "1.655538558959961\n",
      "682\n",
      "1.6228530406951904\n",
      "683\n",
      "1.6436238288879395\n",
      "684\n",
      "1.642714500427246\n",
      "685\n",
      "1.629834771156311\n",
      "686\n",
      "1.5661007165908813\n",
      "687\n",
      "1.5504237413406372\n",
      "688\n",
      "1.5079405307769775\n",
      "689\n",
      "1.4926362037658691\n",
      "690\n",
      "1.4827215671539307\n",
      "691\n",
      "1.4795300960540771\n",
      "692\n",
      "1.5244072675704956\n",
      "693\n",
      "1.5506725311279297\n",
      "694\n",
      "1.5656063556671143\n",
      "695\n",
      "1.6153007745742798\n",
      "696\n",
      "1.6354849338531494\n",
      "697\n",
      "2.4631519317626953\n",
      "698\n",
      "2.5146422386169434\n",
      "699\n",
      "2.506159782409668\n",
      "700\n",
      "2.5692949295043945\n",
      "701\n",
      "2.8548099994659424\n",
      "702\n",
      "3.034471035003662\n",
      "703\n",
      "3.235584259033203\n",
      "704\n",
      "3.4671435356140137\n",
      "705\n",
      "3.960361957550049\n",
      "706\n",
      "4.043221473693848\n",
      "707\n",
      "4.66579008102417\n",
      "708\n",
      "4.055760860443115\n",
      "709\n",
      "3.3754630088806152\n",
      "710\n",
      "3.1874327659606934\n",
      "711\n",
      "2.7372658252716064\n",
      "712\n",
      "2.67167329788208\n",
      "713\n",
      "2.530877113342285\n",
      "714\n",
      "2.5444302558898926\n",
      "715\n",
      "1.9337894916534424\n",
      "716\n",
      "1.8239761590957642\n",
      "717\n",
      "1.77428138256073\n",
      "718\n",
      "1.7472807168960571\n",
      "719\n",
      "1.7422764301300049\n",
      "720\n",
      "1.73193359375\n",
      "721\n",
      "1.7021327018737793\n",
      "722\n",
      "1.701446294784546\n",
      "723\n",
      "1.6630122661590576\n",
      "724\n",
      "1.6199572086334229\n",
      "725\n",
      "1.6135717630386353\n",
      "726\n",
      "1.568504810333252\n",
      "727\n",
      "1.5763899087905884\n",
      "728\n",
      "1.608689308166504\n",
      "729\n",
      "1.6116626262664795\n",
      "730\n",
      "1.505763053894043\n",
      "731\n",
      "1.5004608631134033\n",
      "732\n",
      "1.4669681787490845\n",
      "733\n",
      "1.5053033828735352\n",
      "734\n",
      "1.489970088005066\n",
      "735\n",
      "1.5228394269943237\n",
      "736\n",
      "1.5370402336120605\n",
      "737\n",
      "1.5716938972473145\n",
      "738\n",
      "1.5498449802398682\n",
      "739\n",
      "1.5837947130203247\n",
      "740\n",
      "1.5857864618301392\n",
      "741\n",
      "1.5951964855194092\n",
      "742\n",
      "1.6170480251312256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n",
      "1.6406278610229492\n",
      "744\n",
      "1.6354706287384033\n",
      "745\n",
      "1.6133848428726196\n",
      "746\n",
      "1.615797758102417\n",
      "747\n",
      "1.6877093315124512\n",
      "748\n",
      "1.766562581062317\n",
      "749\n",
      "1.7721657752990723\n",
      "750\n",
      "1.6527771949768066\n",
      "751\n",
      "1.6301953792572021\n",
      "752\n",
      "1.5818414688110352\n",
      "753\n",
      "1.5953447818756104\n",
      "754\n",
      "1.5801844596862793\n",
      "755\n",
      "1.6258811950683594\n",
      "756\n",
      "1.5803049802780151\n",
      "757\n",
      "1.5971773862838745\n",
      "758\n",
      "1.5994346141815186\n",
      "759\n",
      "1.663081169128418\n",
      "760\n",
      "1.6789071559906006\n",
      "761\n",
      "1.7339739799499512\n",
      "762\n",
      "1.747467279434204\n",
      "763\n",
      "1.976086139678955\n",
      "764\n",
      "2.0801711082458496\n",
      "765\n",
      "2.160717725753784\n",
      "766\n",
      "2.135286331176758\n",
      "767\n",
      "2.181361675262451\n",
      "768\n",
      "2.111454725265503\n",
      "769\n",
      "2.4174013137817383\n",
      "770\n",
      "2.382416248321533\n",
      "771\n",
      "2.5930001735687256\n",
      "772\n",
      "2.703896999359131\n",
      "773\n",
      "2.9800610542297363\n",
      "774\n",
      "3.0629587173461914\n",
      "775\n",
      "3.0777907371520996\n",
      "776\n",
      "2.9506235122680664\n",
      "777\n",
      "2.9673383235931396\n",
      "778\n",
      "2.727315664291382\n",
      "779\n",
      "2.8210010528564453\n",
      "780\n",
      "3.049732208251953\n",
      "781\n",
      "3.1418375968933105\n",
      "782\n",
      "3.866699695587158\n",
      "783\n",
      "3.9865994453430176\n",
      "784\n",
      "4.382372856140137\n",
      "785\n",
      "4.404534816741943\n",
      "786\n",
      "3.8419060707092285\n",
      "787\n",
      "3.639026165008545\n",
      "788\n",
      "3.539536476135254\n",
      "789\n",
      "3.6572136878967285\n",
      "790\n",
      "3.1021337509155273\n",
      "791\n",
      "3.048274040222168\n",
      "792\n",
      "2.634554624557495\n",
      "793\n",
      "2.700000047683716\n",
      "794\n",
      "2.6837551593780518\n",
      "795\n",
      "2.7411553859710693\n",
      "796\n",
      "2.8713464736938477\n",
      "797\n",
      "2.7785191535949707\n",
      "798\n",
      "3.0454676151275635\n",
      "799\n",
      "3.3745274543762207\n",
      "800\n",
      "3.516719341278076\n",
      "801\n",
      "3.4814538955688477\n",
      "802\n",
      "3.1186680793762207\n",
      "803\n",
      "2.5823636054992676\n",
      "804\n",
      "2.510838031768799\n",
      "805\n",
      "2.4279394149780273\n",
      "806\n",
      "2.329604387283325\n",
      "807\n",
      "2.008272647857666\n",
      "808\n",
      "1.9259309768676758\n",
      "809\n",
      "1.9082298278808594\n",
      "810\n",
      "1.881044864654541\n",
      "811\n",
      "1.8801546096801758\n",
      "812\n",
      "1.9185909032821655\n",
      "813\n",
      "1.9270689487457275\n",
      "814\n",
      "1.9008369445800781\n",
      "815\n",
      "1.9370813369750977\n",
      "816\n",
      "1.910401463508606\n",
      "817\n",
      "1.9002115726470947\n",
      "818\n",
      "1.8903878927230835\n",
      "819\n",
      "1.8995072841644287\n",
      "820\n",
      "1.9029338359832764\n",
      "821\n",
      "1.912210464477539\n",
      "822\n",
      "1.888288140296936\n",
      "823\n",
      "1.8935134410858154\n",
      "824\n",
      "1.8390703201293945\n",
      "825\n",
      "1.893157720565796\n",
      "826\n",
      "1.9133906364440918\n",
      "827\n",
      "2.0000691413879395\n",
      "828\n",
      "2.1841888427734375\n",
      "829\n",
      "2.2532448768615723\n",
      "830\n",
      "2.3032405376434326\n",
      "831\n",
      "2.351433277130127\n",
      "832\n",
      "2.3071556091308594\n",
      "833\n",
      "2.36715030670166\n",
      "834\n",
      "2.4331297874450684\n",
      "835\n",
      "2.8351521492004395\n",
      "836\n",
      "3.3362507820129395\n",
      "837\n",
      "3.2570128440856934\n",
      "838\n",
      "3.656909465789795\n",
      "839\n",
      "3.4153013229370117\n",
      "840\n",
      "2.815039873123169\n",
      "841\n",
      "2.7216057777404785\n",
      "842\n",
      "2.6173110008239746\n",
      "843\n",
      "2.4367918968200684\n",
      "844\n",
      "1.8515645265579224\n",
      "845\n",
      "1.7383753061294556\n",
      "846\n",
      "1.735668659210205\n",
      "847\n",
      "1.7519311904907227\n",
      "848\n",
      "1.7869280576705933\n",
      "849\n",
      "1.8440276384353638\n",
      "850\n",
      "1.8079713582992554\n",
      "851\n",
      "1.9846930503845215\n",
      "852\n",
      "1.9645432233810425\n",
      "853\n",
      "2.053821563720703\n",
      "854\n",
      "2.0228378772735596\n",
      "855\n",
      "1.9831428527832031\n",
      "856\n",
      "1.8927685022354126\n",
      "857\n",
      "2.154409885406494\n",
      "858\n",
      "2.0782148838043213\n",
      "859\n",
      "2.053898334503174\n",
      "860\n",
      "2.0650219917297363\n",
      "861\n",
      "2.1137466430664062\n",
      "862\n",
      "2.201333999633789\n",
      "863\n",
      "2.348353624343872\n",
      "864\n",
      "2.4160690307617188\n",
      "865\n",
      "2.5402486324310303\n",
      "866\n",
      "2.4047083854675293\n",
      "867\n",
      "2.096266984939575\n",
      "868\n",
      "2.052647113800049\n",
      "869\n",
      "2.0236973762512207\n",
      "870\n",
      "2.1134579181671143\n",
      "871\n",
      "2.174379348754883\n",
      "872\n",
      "2.1268858909606934\n",
      "873\n",
      "2.303931474685669\n",
      "874\n",
      "2.2651379108428955\n",
      "875\n",
      "2.3780627250671387\n",
      "876\n",
      "2.503943920135498\n",
      "877\n",
      "2.525968074798584\n",
      "878\n",
      "2.6321167945861816\n",
      "879\n",
      "2.7242045402526855\n",
      "880\n",
      "2.6196560859680176\n",
      "881\n",
      "2.657627582550049\n",
      "882\n",
      "2.5662460327148438\n",
      "883\n",
      "2.891878604888916\n",
      "884\n",
      "2.9445595741271973\n",
      "885\n",
      "3.0315399169921875\n",
      "886\n",
      "3.0414834022521973\n",
      "887\n",
      "2.4347152709960938\n",
      "888\n",
      "2.2986249923706055\n",
      "889\n",
      "2.147486925125122\n",
      "890\n",
      "2.1125974655151367\n",
      "891\n",
      "2.0905702114105225\n",
      "892\n",
      "2.6279139518737793\n",
      "893\n",
      "2.7755186557769775\n",
      "894\n",
      "2.9439618587493896\n",
      "895\n",
      "3.030515193939209\n",
      "896\n",
      "3.1037368774414062\n",
      "897\n",
      "3.107673168182373\n",
      "898\n",
      "3.4195451736450195\n",
      "899\n",
      "3.483499050140381\n",
      "900\n",
      "4.826404094696045\n",
      "901\n",
      "5.261541843414307\n",
      "902\n",
      "5.849442958831787\n",
      "903\n",
      "6.462918281555176\n",
      "904\n",
      "4.016639232635498\n",
      "905\n",
      "4.158393383026123\n",
      "906\n",
      "3.2753987312316895\n",
      "907\n",
      "3.430020332336426\n",
      "908\n",
      "3.504274368286133\n",
      "909\n",
      "3.4293923377990723\n",
      "910\n",
      "4.148090839385986\n",
      "911\n",
      "3.886445999145508\n",
      "912\n",
      "3.325244903564453\n",
      "913\n",
      "3.670473575592041\n",
      "914\n",
      "4.109045028686523\n",
      "915\n",
      "4.561017036437988\n",
      "916\n",
      "4.903694152832031\n",
      "917\n",
      "4.990688800811768\n",
      "918\n",
      "4.209125995635986\n",
      "919\n",
      "4.230658531188965\n",
      "920\n",
      "3.835451602935791\n",
      "921\n",
      "3.4074273109436035\n",
      "922\n",
      "2.8196353912353516\n",
      "923\n",
      "2.7475509643554688\n",
      "924\n",
      "2.5122487545013428\n",
      "925\n",
      "2.3139801025390625\n",
      "926\n",
      "1.9305651187896729\n",
      "927\n",
      "1.9109549522399902\n",
      "928\n",
      "1.3493056297302246\n",
      "929\n",
      "1.2846437692642212\n",
      "930\n",
      "1.2388511896133423\n",
      "931\n",
      "1.255129098892212\n",
      "932\n",
      "1.243401288986206\n",
      "933\n",
      "1.249323844909668\n",
      "934\n",
      "1.2434064149856567\n",
      "935\n",
      "1.279598355293274\n",
      "936\n",
      "1.3132656812667847\n",
      "937\n",
      "1.318860411643982\n",
      "938\n",
      "1.3362483978271484\n",
      "939\n",
      "1.333483099937439\n",
      "940\n",
      "1.3522565364837646\n",
      "941\n",
      "1.3548948764801025\n",
      "942\n",
      "1.3715131282806396\n",
      "943\n",
      "1.399771809577942\n",
      "944\n",
      "1.3967127799987793\n",
      "945\n",
      "1.3724894523620605\n",
      "946\n",
      "1.3637615442276\n",
      "947\n",
      "1.335839867591858\n",
      "948\n",
      "1.3501667976379395\n",
      "949\n",
      "1.3695875406265259\n",
      "950\n",
      "1.386383295059204\n",
      "951\n",
      "1.425331950187683\n",
      "952\n",
      "1.4320276975631714\n",
      "953\n",
      "1.4708043336868286\n",
      "954\n",
      "1.4046337604522705\n",
      "955\n",
      "1.4201459884643555\n",
      "956\n",
      "1.4100761413574219\n",
      "957\n",
      "2.1953821182250977\n",
      "958\n",
      "2.301396369934082\n",
      "959\n",
      "2.209333658218384\n",
      "960\n",
      "2.5193543434143066\n",
      "961\n",
      "2.7748851776123047\n",
      "962\n",
      "2.993602752685547\n",
      "963\n",
      "2.9952392578125\n",
      "964\n",
      "3.003086566925049\n",
      "965\n",
      "3.523650646209717\n",
      "966\n",
      "3.821871280670166\n",
      "967\n",
      "4.722366809844971\n",
      "968\n",
      "4.827958106994629\n",
      "969\n",
      "4.395632266998291\n",
      "970\n",
      "3.6513729095458984\n",
      "971\n",
      "3.3341622352600098\n",
      "972\n",
      "3.1738314628601074\n",
      "973\n",
      "2.9454565048217773\n",
      "974\n",
      "2.745785713195801\n",
      "975\n",
      "2.5018491744995117\n",
      "976\n",
      "2.1637163162231445\n",
      "977\n",
      "2.385263204574585\n",
      "978\n",
      "2.2855348587036133\n",
      "979\n",
      "2.462529420852661\n",
      "980\n",
      "2.352799415588379\n",
      "981\n",
      "2.3837029933929443\n",
      "982\n",
      "2.315582513809204\n",
      "983\n",
      "2.199495792388916\n",
      "984\n",
      "2.1057517528533936\n",
      "985\n",
      "2.0251007080078125\n",
      "986\n",
      "1.9094899892807007\n",
      "987\n",
      "1.8487536907196045\n",
      "988\n",
      "1.8547015190124512\n",
      "989\n",
      "1.8886247873306274\n",
      "990\n",
      "1.853013515472412\n",
      "991\n",
      "1.830552101135254\n",
      "992\n",
      "1.7839555740356445\n",
      "993\n",
      "1.7608437538146973\n",
      "994\n",
      "1.7541935443878174\n",
      "995\n",
      "1.8124809265136719\n",
      "996\n",
      "1.7878286838531494\n",
      "997\n",
      "1.8643205165863037\n",
      "998\n",
      "1.850006103515625\n",
      "999\n",
      "1.8159666061401367\n",
      "1000\n",
      "1.8145372867584229\n",
      "1001\n",
      "1.8633519411087036\n",
      "1002\n",
      "1.7987060546875\n",
      "1003\n",
      "1.7951656579971313\n",
      "1004\n",
      "1.7456505298614502\n",
      "1005\n",
      "1.7370259761810303\n",
      "1006\n",
      "1.772836685180664\n",
      "1007\n",
      "1.7895445823669434\n",
      "1008\n",
      "1.829279899597168\n",
      "1009\n",
      "1.8641295433044434\n",
      "1010\n",
      "1.9254350662231445\n",
      "1011\n",
      "1.9590766429901123\n",
      "1012\n",
      "1.9633004665374756\n",
      "1013\n",
      "2.02193021774292\n",
      "1014\n",
      "2.0313260555267334\n",
      "1015\n",
      "2.127559185028076\n",
      "1016\n",
      "2.086571216583252\n",
      "1017\n",
      "2.1269497871398926\n",
      "1018\n",
      "2.1461808681488037\n",
      "1019\n",
      "2.2497029304504395\n",
      "1020\n",
      "2.2040295600891113\n",
      "1021\n",
      "2.090522050857544\n",
      "1022\n",
      "1.9262932538986206\n",
      "1023\n",
      "1.9573006629943848\n",
      "1024\n",
      "1.9018113613128662\n",
      "1025\n",
      "1.9750906229019165\n",
      "1026\n",
      "2.1389894485473633\n",
      "1027\n",
      "2.19648814201355\n",
      "1028\n",
      "2.2147059440612793\n",
      "1029\n",
      "2.1117281913757324\n",
      "1030\n",
      "2.112475872039795\n",
      "1031\n",
      "2.0574772357940674\n",
      "8\n",
      "6.291419982910156\n",
      "9\n",
      "7.186431884765625\n",
      "10\n",
      "7.27117395401001\n",
      "11\n",
      "8.113638877868652\n",
      "12\n",
      "8.400055885314941\n",
      "13\n",
      "8.876890182495117\n",
      "14\n",
      "9.653185844421387\n",
      "15\n",
      "9.396834373474121\n",
      "16\n",
      "11.285517692565918\n",
      "17\n",
      "11.585834503173828\n",
      "18\n",
      "11.407512664794922\n",
      "19\n",
      "10.713775634765625\n",
      "20\n",
      "10.825111389160156\n",
      "21\n",
      "10.95455265045166\n",
      "22\n",
      "8.59669303894043\n",
      "23\n",
      "7.274786949157715\n",
      "24\n",
      "6.4676289558410645\n",
      "25\n",
      "5.448660850524902\n",
      "26\n",
      "4.743436336517334\n",
      "27\n",
      "4.46619176864624\n",
      "28\n",
      "4.294191360473633\n",
      "29\n",
      "3.9281444549560547\n",
      "30\n",
      "3.698551654815674\n",
      "31\n",
      "3.4856162071228027\n",
      "32\n",
      "3.2208251953125\n",
      "33\n",
      "3.254427909851074\n",
      "34\n",
      "3.2158327102661133\n",
      "35\n",
      "3.1246109008789062\n",
      "36\n",
      "3.1307802200317383\n",
      "37\n",
      "3.2214975357055664\n",
      "38\n",
      "3.1900768280029297\n",
      "39\n",
      "3.26119327545166\n",
      "40\n",
      "3.2069759368896484\n",
      "41\n",
      "3.3651599884033203\n",
      "42\n",
      "3.353227138519287\n",
      "43\n",
      "3.2321176528930664\n",
      "44\n",
      "3.238062858581543\n",
      "45\n",
      "3.251377582550049\n",
      "46\n",
      "3.184244155883789\n",
      "47\n",
      "2.9895079135894775\n",
      "48\n",
      "2.9457051753997803\n",
      "49\n",
      "2.895094156265259\n",
      "50\n",
      "2.8271632194519043\n",
      "51\n",
      "2.8140201568603516\n",
      "52\n",
      "2.944607734680176\n",
      "53\n",
      "2.9807660579681396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     patch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(patch)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     67\u001b[0m     patch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(patch, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m     pred[p\u001b[38;5;241m-\u001b[39m(res\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), q\u001b[38;5;241m-\u001b[39m(res\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred[p\u001b[38;5;241m-\u001b[39m(res\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m500\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mMyCNNModel.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mMySepConvLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x_sep_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_conv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     x_sep_conv_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep_conv_block(x_sep_conv) \u001b[38;5;66;03m# performs second SepConv, see Lang et al. (2019), p. 6\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = 15\n",
    "res = int((res-1)/2)\n",
    "model = torch.load(\"model01.pth\")\n",
    "model.eval()\n",
    "for i in range(2):\n",
    "    image = np.load(f\"test_images/image_00{i}.npy\")\n",
    "    image = np.transpose(image, (1,2,0))\n",
    "    \n",
    "    \n",
    "    nan_values_before = (np.count_nonzero(np.isnan(image)))\n",
    "            \n",
    "    channel8 = image[:, :, 6]\n",
    "    channel4 = image[:, :, 2]\n",
    "    channels = image.shape\n",
    "    width = image[0].shape[0]\n",
    "    height = image[0].shape[1]\n",
    "\n",
    "    # add the vegetation array \n",
    "    vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(np.add(channel8, channel4), 1e-6))\n",
    "            \n",
    "    nan_values_vegetation = (np.count_nonzero(np.isnan(vegetation_array)))\n",
    "            \n",
    "    if(nan_values_vegetation > 0): \n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy had\", nan_values_before, \"before vegetation index\")\n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy has\", nan_values_vegetation, \"nan_values after adding vegetation\")\n",
    "            \n",
    "            \n",
    "    vegetation_array = np.nan_to_num(vegetation_array, nan=0.0)\n",
    "    image_transformed = np.concatenate((image, vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "\n",
    "            \n",
    "    image = image_transformed\n",
    "    nan_values_before = 0\n",
    "    # add moisture index\n",
    "    channel8a = image[:, :, 7]\n",
    "    channel11 = image[:, :, 8]\n",
    "    moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(np.add(channel8a, channel11), 1e-6))\n",
    "            \n",
    "    nan_values_moisture = (np.count_nonzero(np.isnan(moisture_array)))\n",
    "            \n",
    "    if(nan_values_moisture > 0): \n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy had\", nan_values_before, \"before moisture index\")\n",
    "        print(\"picture\",f\"test_images/image_00{i}.npy has\", nan_values_moisture, \"nan_values after adding moisture\")\n",
    "            \n",
    "            \n",
    "    image_transformed = np.concatenate((image,moisture_array[:,:, np.newaxis]), axis = 2)\n",
    "    image = image_transformed\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add padding to every image edge in case there are ground truths which are too close to an edge\n",
    "    padded_image = np.pad(image, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "    # Normalize the image with the means and stds from the trainset\n",
    "    padded_image[:,:,:10] = (padded_image[:,:,:10] - channel_means[:10]) / channel_stds[:10]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    pred = np.zeros((1024, 1024))\n",
    "    for p in range(res+1, 1024+res+1):\n",
    "        for q in range(res+1, 1024+res+1):\n",
    "            patch = padded_image[p-res : p+res+1, q-res : q+res+1, :]\n",
    "            patch = torch.from_numpy(patch).float()\n",
    "            patch = torch.unsqueeze(patch, 0).permute(0,3,1,2)\n",
    "            pred[p-(res+1), q-(res+1)] = model(patch)\n",
    "        print(p)\n",
    "        print(pred[p-(res+1), 500])\n",
    "            \n",
    "    \n",
    "    np.save(f\"test_images/prediction_00{i}.npy\", pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9b4ed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9919929726482906"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred0 = np.load(\"test_images/prediction_000.npy\")\n",
    "pred0 = np.expand_dims(pred0, axis=0)\n",
    "pred0.shape\n",
    "np.mean(pred0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6f2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
