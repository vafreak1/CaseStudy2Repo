{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2442d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d6fef",
   "metadata": {},
   "source": [
    "## First, we slice the train images into 31 x 31 pixels with the ground truth in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndigit(n, x):\n",
    "    x = str(x)\n",
    "    while(len(x) < n):\n",
    "        x = \"0\" + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef62c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(res, files = 20):\n",
    "    j = 0\n",
    "    path = [\"02\", \"train\"]\n",
    "    res = int((res-1)/2)\n",
    "    nan_values = 0\n",
    "    for p in path:\n",
    "        for f in range(files):\n",
    "            image = np.load(f\"images_{p}/images/image_{ndigit(3, f)}.npy\")\n",
    "            mask = np.load(f\"masks_{p}/masks/mask_{ndigit(3, f)}.npy\")\n",
    "            image = np.reshape(image, (1024,1024,10))\n",
    "            mask = np.reshape(mask, (1024,1024,1))\n",
    "            \n",
    "            nan_values_before = (np.count_nonzero(np.isnan(image)))\n",
    "            \n",
    "            channel8 = image[:, :, 7]\n",
    "            channel4 = image[:, :, 3]\n",
    "            channels = image.shape\n",
    "            width = image[0].shape[0]\n",
    "            height = image[0].shape[1]\n",
    "\n",
    "            # add the vegetation array \n",
    "            vegetation_array = np.divide((np.subtract(channel8, channel4)), np.add(np.add(channel8, channel4), .0001))\n",
    "            vegetation_array = np.nan_to_num(vegetation_array, nan=0.0)\n",
    "            image_transformed = np.concatenate((image, vegetation_array[:, :, np.newaxis]), axis=2)\n",
    "\n",
    "        \n",
    "            image = image_transformed\n",
    "            nan_values_before = 0\n",
    "            # add moisture index\n",
    "            channel8a = image[:, :, 7]\n",
    "            channel11 = image[:, :, 8]\n",
    "            moisture_array = np.divide((np.subtract(channel8a, channel11)), np.add(np.add(channel8a, channel11), .0001))\n",
    "            \n",
    "            nan_values_moisture = (np.count_nonzero(np.isnan(moisture_array)))\n",
    "            \n",
    "            if(nan_values_moisture > 0): \n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy had\", nan_values_before, \"before moisture index\")\n",
    "                print(\"picture\",f\"images_{p}/images/image_{ndigit(3, f)}.npy has\", nan_values_moisture, \"nan_values after adding moisture\")\n",
    "            \n",
    "            # fill nan values with 0\n",
    "            moisture_array = np.nan_to_num(moisture_array, nan=0.0)\n",
    "            \n",
    "            image_transformed = np.concatenate((image,moisture_array[:,:, np.newaxis]), axis = 2)\n",
    "            image = image_transformed\n",
    "\n",
    "            nan_values_pic = np.count_nonzero(np.isnan(image))\n",
    "            nan_values += nan_values_pic\n",
    "            # Add padding to every image (and mask) edge in case there are ground truths which are too close to an edge\n",
    "            padded_image = np.pad(image, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "            padded_mask = np.pad(mask, ((res+1, res+1), (res+1, res+1), (0,0)), mode='constant')\n",
    "\n",
    "            # Extract ground truths\n",
    "            ground_truths_pos = np.array(np.where(padded_mask != 0)).T\n",
    "            \n",
    "            # Slice and save patches around each ground truth\n",
    "            for i in ground_truths_pos: \n",
    "                patch = (padded_image[i[0]-res : i[0]+res+1, i[1]-res : i[1]+res+1, :], padded_mask[i[0], i[1], 0])\n",
    "                np.save(f\"patches/train/patch_{p}_{ndigit(3, f)}_{ndigit(5, j)}.npy\", np.array(patch, dtype=\"object\"))                                 \n",
    "                j += 1\n",
    "    print(\"Added Vegetation (B8-B4)/(B8+B4)\")\n",
    "    print(\"Added Moisture (B8A-B11)/(B8A+B11)\")\n",
    "    print(\"Patched the pictures\")\n",
    "    print(\"NaN values:\", nan_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31235460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of ground truths\n",
    "# pos = 0\n",
    "# for i in range(20):\n",
    "#     mask = np.load(f\"masks_02/masks/mask_{ndigit(3, i)}.npy\")\n",
    "#     ground_truths_pos = np.array(np.where(mask != 0)).T\n",
    "#     pos = pos + len(ground_truths_pos)\n",
    "# print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fce53c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Vegetation (B8-B4)/(B8+B4)\n",
      "Added Moisture (B8A-B11)/(B8A+B11)\n",
      "Patched the pictures\n",
      "NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "res = 15\n",
    "load_data(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b7cd4",
   "metadata": {},
   "source": [
    "## Then, we load the data and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca66217",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b0bff0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatches/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m trainset0 \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(file_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m      5\u001b[0m trainset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pic \u001b[38;5;129;01min\u001b[39;00m trainset0:\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatches/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m trainset0 \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m      5\u001b[0m trainset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pic \u001b[38;5;129;01min\u001b[39;00m trainset0:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load patches\n",
    "directory = 'patches/train'\n",
    "file_paths = glob.glob(directory + '/*.npy')\n",
    "trainset0 = [np.load(file_path, allow_pickle=True) for file_path in file_paths]\n",
    "trainset = []\n",
    "for pic in trainset0:\n",
    "    trainset.append(pic)\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d208fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_enriched = trainset\n",
    "print(trainset_enriched[0])\n",
    "#len(trainset_enriched)\n",
    "#trainset[pic_no][0][h][w][channel] -> pixel value\n",
    "#trainset[pic_no][1] -> Ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541939c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = trainset_enriched[0]\n",
    "X.shape\n",
    "trainset_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a187451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, trainset, transform):\n",
    "        self.trainset = trainset\n",
    "        self.transform = transform\n",
    "       #self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trainset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.trainset[index]\n",
    "\n",
    "        # apply each transformation jointly to each input\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # apply each augmentation separately to each input\n",
    "        #if self.augmentations:\n",
    "        #    for augmentation in self.augmentations:\n",
    "        #        data = augmentation(data)\n",
    "\n",
    "\n",
    "        return data, target\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.ConvertImageDtype(torch.float64),\n",
    "     transforms.Lambda(lambda x : x / 3000),\n",
    "     transforms.Lambda(lambda x : torch.where(x > 1, 1, x)), # clip images between 0 and 1\n",
    "     transforms.Normalize(mean=(0.5,)*12,\n",
    "                          std=(0.5,)*12)\n",
    "     ])\n",
    "\n",
    "# augmentations = [\n",
    "#      transforms.RandomRotation(360),\n",
    "#      transforms.RandomAffine(degrees=0, translate=(0.5,0.5)), # shift in both directions along 0.5 * height on y-axis and 0.5 * width on x-axis\n",
    "#      transforms.RandomAffine(0, scale=(10,45)), # scale in range 10 <= scale <= 45\n",
    "#      transforms.RandomAffine(0, shear=[10,30,10,30]), # shear on x- and y-axis between (10,30) \n",
    "#      transforms.ElasticTransform(alpha=50.0, sigma=3.0) # displaces pixels\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom dataset\n",
    "trainset_transformed = CustomDataset(trainset_enriched, transform=transform)\n",
    "trainset_transformed[0][0]\n",
    "#on the fly augmentation during training, hence no additional pictures in trainset \n",
    "#len(trainset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8f4492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31090 7773\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of the training set and validation set\n",
    "train_size = int(0.8 * len(trainset_transformed))\n",
    "val_size = len(trainset_transformed) - train_size\n",
    "\n",
    "# Split trainset into trainset and valset\n",
    "trainset_load, valset_load = random_split(trainset_transformed, [train_size, val_size])\n",
    "print(len(trainset_load), len(valset_load))\n",
    "\n",
    "# Create data loaders for the training set and validation set\n",
    "trainloader = DataLoader(trainset_load, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valset_load, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e297aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-0.2820, -0.2540, -0.0807,  ...,  0.3527,  0.3093, -0.0673],\n",
      "          [-0.2800, -0.3913, -0.4380,  ...,  0.2993,  0.3813,  0.4567],\n",
      "          [-0.2527, -0.4520, -0.3493,  ...,  0.3893,  0.4527,  0.4207],\n",
      "          ...,\n",
      "          [-0.3187, -0.2667, -0.2860,  ...,  0.4493,  0.7293,  0.4833],\n",
      "          [-0.4253, -0.4093, -0.3647,  ...,  0.5773,  0.7640,  0.5827],\n",
      "          [-0.2133, -0.3687, -0.2640,  ...,  0.3087,  0.8207,  0.4447]],\n",
      "\n",
      "         [[-0.2820, -0.2540, -0.0807,  ...,  0.3493,  0.3147,  0.1487],\n",
      "          [-0.2720, -0.4233, -0.4173,  ...,  0.2993,  0.3813,  0.4567],\n",
      "          [-0.2527, -0.4520, -0.3493,  ...,  0.3893,  0.4093,  0.4333],\n",
      "          ...,\n",
      "          [-0.3187, -0.2667, -0.2860,  ...,  0.4493,  0.7293,  0.4833],\n",
      "          [-0.4093, -0.3833, -0.3893,  ...,  0.5267,  0.6480,  0.6233],\n",
      "          [-0.3553, -0.3620, -0.1593,  ...,  0.3087,  0.8207,  0.4447]],\n",
      "\n",
      "         [[-0.2460, -0.3233, -0.1553,  ...,  0.3493,  0.3147,  0.1487],\n",
      "          [-0.2720, -0.4233, -0.4173,  ...,  0.3313,  0.3173,  0.4960],\n",
      "          [-0.2467, -0.4127, -0.3873,  ...,  0.8287,  0.4093,  0.4333],\n",
      "          ...,\n",
      "          [-0.2660, -0.3500, -0.2847,  ...,  0.3953,  0.6193,  0.4280],\n",
      "          [-0.4093, -0.3833, -0.3893,  ...,  0.5267,  0.6480,  0.6233],\n",
      "          [-0.3553, -0.3620, -0.1593,  ...,  0.5227,  0.9067,  0.5373]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2280, -0.2707,  0.3980,  ...,  0.3093, -0.0673,  0.5067],\n",
      "          [-0.3913, -0.4380,  0.2220,  ...,  0.3833,  0.3880,  0.6453],\n",
      "          [-0.4047, -0.2253, -0.2893,  ...,  0.4527,  0.4207,  0.6393],\n",
      "          ...,\n",
      "          [-0.1280, -0.3287, -0.3440,  ...,  0.7760,  0.6053,  0.5433],\n",
      "          [-0.4093, -0.3647, -0.3820,  ...,  0.7640,  0.5827,  0.5427],\n",
      "          [-0.3687, -0.2640, -0.3987,  ...,  0.5493,  0.4333,  0.5533]],\n",
      "\n",
      "         [[-1.0000, -1.0001, -1.0000,  ..., -1.0000, -1.0001, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9999,  ..., -1.0000, -1.0000, -0.9999],\n",
      "          [-1.0001, -0.9999, -1.0000,  ..., -1.0000, -1.0000, -0.9999],\n",
      "          ...,\n",
      "          [-0.9999, -1.0000, -0.9999,  ..., -0.9999, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9999,  ..., -0.9999, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0001, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0001, -1.0001,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-0.9999, -1.0000, -0.9999,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.8120, -0.8000, -0.7873,  ..., -0.5093, -0.3867, -0.1260],\n",
      "          [-0.7960, -0.7960, -0.8127,  ..., -0.6720, -0.7047, -0.2213],\n",
      "          [-0.8133, -0.8007, -0.8353,  ..., -0.4353, -0.6473, -0.6907],\n",
      "          ...,\n",
      "          [-0.2913, -0.3407, -0.4293,  ..., -0.4360, -0.5613, -0.4353],\n",
      "          [-0.3500, -0.4027, -0.4873,  ..., -0.4453, -0.4753, -0.4207],\n",
      "          [-0.6147, -0.4287, -0.4653,  ..., -0.7307, -0.6820, -0.6480]],\n",
      "\n",
      "         [[-0.7987, -0.7987, -0.7873,  ..., -0.4513, -0.3367, -0.1373],\n",
      "          [-0.7873, -0.7987, -0.7973,  ..., -0.6887, -0.6873, -0.1547],\n",
      "          [-0.8313, -0.8047, -0.8093,  ..., -0.4580, -0.6593, -0.7687],\n",
      "          ...,\n",
      "          [-0.3033, -0.3560, -0.4887,  ..., -0.5020, -0.5547, -0.4113],\n",
      "          [-0.3667, -0.4047, -0.4700,  ..., -0.4773, -0.4827, -0.4167],\n",
      "          [-0.4920, -0.4400, -0.4200,  ..., -0.7380, -0.6927, -0.6533]],\n",
      "\n",
      "         [[-0.7953, -0.7927, -0.7880,  ..., -0.4347, -0.2913, -0.1747],\n",
      "          [-0.7940, -0.7913, -0.7907,  ..., -0.6780, -0.7167, -0.1533],\n",
      "          [-0.8200, -0.7993, -0.7920,  ..., -0.4807, -0.6660, -0.7767],\n",
      "          ...,\n",
      "          [-0.3267, -0.3533, -0.5220,  ..., -0.4520, -0.5673, -0.4073],\n",
      "          [-0.4147, -0.4253, -0.4773,  ..., -0.5813, -0.4867, -0.4067],\n",
      "          [-0.4093, -0.4300, -0.4420,  ..., -0.7253, -0.6853, -0.6507]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7900, -0.7700, -0.6293,  ..., -0.1567, -0.1313, -0.6373],\n",
      "          [-0.8033, -0.8213, -0.7820,  ..., -0.6833, -0.1933, -0.7087],\n",
      "          [-0.7893, -0.8233, -0.7907,  ..., -0.6613, -0.6653, -0.6740],\n",
      "          ...,\n",
      "          [-0.3960, -0.4787, -0.6267,  ..., -0.5120, -0.4373, -0.3800],\n",
      "          [-0.4207, -0.4907, -0.4307,  ..., -0.5127, -0.4260, -0.4660],\n",
      "          [-0.4047, -0.4887, -0.4500,  ..., -0.6867, -0.6540, -0.6180]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.9998,  ..., -0.9998, -0.9998, -1.0003],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -0.9999, -1.0002],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0001, -1.0000, -0.9999],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0001,  ..., -1.0000, -0.9999, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0002, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0001,  ..., -1.0000, -1.0000, -1.0001],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0001, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -0.9999, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0001, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0167,  0.1240,  0.0020,  ..., -0.1827,  0.0280,  0.2700],\n",
      "          [ 0.2933,  0.4927, -0.0567,  ...,  0.1813,  0.0533, -0.2040],\n",
      "          [-0.4960, -0.2200, -0.4187,  ..., -0.3247, -0.2927, -0.3247],\n",
      "          ...,\n",
      "          [ 0.3473, -0.4047, -0.4513,  ..., -0.4200, -0.4467, -0.5533],\n",
      "          [-0.0113, -0.2587, -0.4493,  ..., -0.3860, -0.4140, -0.4040],\n",
      "          [-0.1300, -0.2287, -0.3207,  ..., -0.4520, -0.4040, -0.4293]],\n",
      "\n",
      "         [[-0.0153,  0.0153,  0.0033,  ..., -0.2560,  0.0813,  0.3573],\n",
      "          [ 0.3660,  0.4187, -0.2093,  ...,  0.1093, -0.0427, -0.2853],\n",
      "          [-0.4733, -0.3647, -0.3760,  ..., -0.3300, -0.1640, -0.4047],\n",
      "          ...,\n",
      "          [ 0.2053, -0.4373, -0.3573,  ..., -0.4060, -0.4533, -0.4227],\n",
      "          [-0.0280, -0.4113, -0.3833,  ..., -0.3720, -0.4200, -0.4113],\n",
      "          [-0.0993, -0.2273, -0.2580,  ..., -0.4687, -0.4233, -0.4447]],\n",
      "\n",
      "         [[-0.0153,  0.0153,  0.0033,  ..., -0.2560,  0.0813,  0.3573],\n",
      "          [ 0.3660,  0.4187, -0.2093,  ...,  0.1093, -0.0427, -0.2853],\n",
      "          [-0.4733, -0.3647, -0.3760,  ..., -0.3300, -0.1640, -0.4047],\n",
      "          ...,\n",
      "          [ 0.2053, -0.4373, -0.3573,  ..., -0.4060, -0.4533, -0.4227],\n",
      "          [-0.0280, -0.4113, -0.3833,  ..., -0.3720, -0.4200, -0.4113],\n",
      "          [-0.0993, -0.2273, -0.2580,  ..., -0.4687, -0.4233, -0.4447]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1240,  0.0020, -0.2747,  ...,  0.0280,  0.2700,  0.2393],\n",
      "          [ 0.4927, -0.0567,  0.0887,  ...,  0.0533, -0.2040, -0.4167],\n",
      "          [-0.2200, -0.4187, -0.3367,  ..., -0.2927, -0.3247, -0.0553],\n",
      "          ...,\n",
      "          [-0.4047, -0.4513, -0.4740,  ..., -0.4467, -0.5533, -0.3587],\n",
      "          [-0.2587, -0.4493, -0.1440,  ..., -0.4140, -0.4040, -0.4867],\n",
      "          [-0.2287, -0.3207, -0.4133,  ..., -0.4040, -0.4293, -0.3927]],\n",
      "\n",
      "         [[-0.9999, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9999,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9999, -0.9999, -1.0000,  ..., -1.0000, -1.0000, -0.9999],\n",
      "          ...,\n",
      "          [-1.0001, -1.0000, -1.0001,  ..., -1.0000, -1.0001, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0001, -1.0000, -1.0001,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]],\n",
      "       dtype=torch.float64), tensor([ 2.4300,  3.1700, 14.2300], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "limit = 0\n",
    "for batch in trainloader:\n",
    "\n",
    "    while(limit <1):\n",
    "        print(batch)\n",
    "    #    print(x.shape)\n",
    "    #    print(x.size)\n",
    "    #    print(y.shape)\n",
    "    #    print(y)\n",
    "        limit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab30c0",
   "metadata": {},
   "source": [
    "## Next, we define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(pl.LightningModule): # New! def init(self, layers, lr=0.01, classes=None): super().init() # <- Very important! self.lr = lr self.classes = classes ## Build model self.layers = nn.Sequential(layers) # Create a sequential model\n",
    "\n",
    "    def __init__(self, *layers, classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = 0.01  # Assign the learning rate here\n",
    "        self.classes = classes\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)  # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_prefix='train'):\n",
    "        X, y = batch\n",
    "        print(X[0].shape, \"X\")\n",
    "        print(y[0].shape, \"y\")\n",
    "        y_hat = self(X)\n",
    "        if(y_hat == None):\n",
    "            return 0\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        #print(y_hat, \"yhat\")\n",
    "        print(y_hat) #[[3,1 5]] 7]] 5]]\n",
    "        #print(y_hat.size)\n",
    "        #print(y_hat.shape)\n",
    "        self.log(f\"{log_prefix}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            return self.training_step(batch, batch_idx, log_prefix='valid')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam with Weight Decay\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        # Simplest scheduler is ReduceLROnPlateau. This scheduler reduces the learning rate by 0.1\n",
    "        # if the val_loss has not decreased within the last 10 epochs.\n",
    "        scheduler = {\n",
    "            # REQUIRED: The scheduler instance\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True),\n",
    "            # The unit of the scheduler's step size, could also be 'step'.\n",
    "            # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "            # updates it after a optimizer update.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"valid_loss\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated, thus stopping\n",
    "            # training if not found. If set to `False`, it will only produce a warning\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a9b5",
   "metadata": {},
   "source": [
    "## Implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements entry to SepConv2d, see Lang et al. (2019), p. 6\n",
    "class MyEntryLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.proj_out = nn.Conv2d(in_channels, out_channels[len(out_channels)-1], (1,1))\n",
    "\n",
    "        self.entry_blocks = nn.ModuleList()\n",
    "        for i in range(len(out_channels)):\n",
    "            self.entry_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels[i], (1, 1)),\n",
    "                nn.BatchNorm2d(out_channels[i]),\n",
    "                nn.ReLU(inplace = True)\n",
    "            ))\n",
    "            in_channels = out_channels[i]  # Update in_channels for next iteration\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_entry = x\n",
    "        for i in range(len(self.out_channels)):\n",
    "            x_entry = self.entry_blocks[i](x_entry)\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements SepConv2D\n",
    "class MySepConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "\n",
    "        self.sep_conv_block = nn.Sequential(\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, groups=in_channels, **kwargs), # depthwise SepConv\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs), # pointwise SepConv\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_sep_conv = self.sep_conv_block(x)\n",
    "        x_sep_conv_2 = self.sep_conv_block(x_sep_conv) # performs second SepConv, see Lang et al. (2019), p. 6\n",
    "        x = self.proj_out(x)\n",
    "        return (x + x_sep_conv_2) # adds original input and sep_conv_2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = MyCNNModel(\n",
    "    #MyEntryLayer(12, [128, 256]), # increase number of channels to 512\n",
    "    nn.Conv2d(12, 256, (3,3), padding='same'),\n",
    "    #MySepConvLayer(256, 256, (3,3), padding='same'),\n",
    "    #MySepConvLayer(256, 256, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(256, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, we need a trainer class\n",
    "from pytorch_lightning.callbacks import RichProgressBar, RichModelSummary\n",
    "trainer1 = pl.Trainer(devices=1, accelerator=\"cpu\", precision='64', max_epochs=1,\n",
    "                      callbacks=[RichProgressBar(refresh_rate=50),\n",
    "                                 RichModelSummary(3),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(tree_model, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b50a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model.eval()\n",
    "tree_model = tree_model.float()\n",
    "batch = next(iter(trainloader))\n",
    "inputs = batch[0]\n",
    "inputs = inputs.float()\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = tree_model(inputs)\n",
    "\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "# expected 128 pictures, each with 15 * 15 predictions for height ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
